"""
è¯Šæ–­è„šæœ¬ - æ£€æŸ¥æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æ˜¯å¦æ­£å¸¸
ç‰¹åˆ«å…³æ³¨ cross-attention æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import torch.nn.functional as F
import os
from config import get_config
from utils import load_model_and_tokenizers


def check_cross_attention(model, src_tensor, tgt_tensor, device):
    """æ£€æŸ¥ cross-attention æƒé‡æ˜¯å¦æ­£å¸¸"""
    print("\nğŸ”¬ Cross-Attention è¯Šæ–­:")
    
    model.eval()
    with torch.no_grad():
        # è·å– encoder è¾“å‡º
        encoder_output, src_mask = model.encode(src_tensor)
        
        print(f"   Encoderè¾“å‡º: shape={encoder_output.shape}, mean={encoder_output.mean():.4f}, std={encoder_output.std():.4f}")
        print(f"   src_mask: shape={src_mask.shape}, Trueæ¯”ä¾‹={src_mask.float().mean():.2%}")
        
        # å¦‚æœæ‰€æœ‰ä½ç½®éƒ½è¢« mask æ‰ï¼Œè¿™æ˜¯ä¸¥é‡é—®é¢˜ï¼
        if src_mask.float().mean() < 0.1:
            print("   âš ï¸ è­¦å‘Š: src_mask ä¸­å¤§éƒ¨åˆ†ä½ç½®è¢« mask æ‰äº†ï¼è¿™ä¼šå¯¼è‡´ cross-attention å¤±æ•ˆï¼")
        
        # æ‰‹åŠ¨æ‰§è¡Œ decoder çš„ä¸€å±‚ï¼Œæ•è· attention weights
        tgt_mask = model.create_decoder_mask(tgt_tensor)
        x = model.tgt_embedding(tgt_tensor)
        
        # é€šè¿‡ç¬¬ä¸€ä¸ª decoder å±‚
        layer = model.decoder_layers[0]
        
        # Self-attention
        self_attn_out, self_attn_weights = layer.self_attention(x, x, x, tgt_mask)
        x = layer.norm1(x + layer.dropout1(self_attn_out))
        
        # Cross-attention - è¿™æ˜¯å…³é”®ï¼
        cross_attn_out, cross_attn_weights = layer.cross_attention(x, encoder_output, encoder_output, src_mask)
        
        print(f"\n   Self-Attention æƒé‡:")
        print(f"     shape: {self_attn_weights.shape}")
        print(f"     mean: {self_attn_weights.mean():.4f}")
        print(f"     max: {self_attn_weights.max():.4f}")
        
        print(f"\n   Cross-Attention æƒé‡ (å…³é”®!):")
        print(f"     shape: {cross_attn_weights.shape}")
        print(f"     mean: {cross_attn_weights.mean():.4f}")
        print(f"     max: {cross_attn_weights.max():.4f}")
        print(f"     min: {cross_attn_weights.min():.4f}")
        
        # æ£€æŸ¥ cross-attention æ˜¯å¦åœ¨çœŸæ­£å…³æ³¨æºæ–‡
        # å¦‚æœæƒé‡éå¸¸å‡åŒ€ï¼ˆæ¥è¿‘ 1/src_lenï¼‰ï¼Œè¯´æ˜æ¨¡å‹æ²¡æœ‰å­¦ä¼šå…³æ³¨
        src_len = src_tensor.size(1)
        uniform_attn = 1.0 / src_len
        
        # è®¡ç®— attention çš„"é›†ä¸­åº¦"ï¼ˆç†µï¼‰
        # ä½ç†µ = é›†ä¸­å…³æ³¨å°‘æ•°ä½ç½®ï¼ˆå¥½ï¼‰
        # é«˜ç†µ = å‡åŒ€åˆ†å¸ƒï¼ˆåï¼Œè¯´æ˜æ²¡å­¦ä¼šï¼‰
        attn_entropy = -(cross_attn_weights * (cross_attn_weights + 1e-10).log()).sum(dim=-1).mean()
        max_entropy = torch.log(torch.tensor(float(src_len)))
        
        print(f"\n   Attention åˆ†æ:")
        print(f"     æºåºåˆ—é•¿åº¦: {src_len}")
        print(f"     å‡åŒ€åˆ†å¸ƒæœŸæœ›å€¼: {uniform_attn:.4f}")
        print(f"     å®é™…ç†µ: {attn_entropy:.4f}")
        print(f"     æœ€å¤§ç†µ (å‡åŒ€åˆ†å¸ƒ): {max_entropy:.4f}")
        print(f"     ç†µæ¯”ä¾‹: {attn_entropy/max_entropy:.2%}")
        
        if attn_entropy / max_entropy > 0.9:
            print("   âš ï¸ Cross-attention æ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼æ¨¡å‹å¯èƒ½æ²¡æœ‰å­¦ä¼šæ­£ç¡®å…³æ³¨æºæ–‡ï¼")
        elif attn_entropy / max_entropy > 0.7:
            print("   âš ï¸ Cross-attention åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ï¼Œæ¨¡å‹è¿˜åœ¨æ—©æœŸå­¦ä¹ é˜¶æ®µ")
        else:
            print("   âœ… Cross-attention æœ‰ä¸€å®šçš„é›†ä¸­åº¦ï¼Œæ¨¡å‹åœ¨å­¦ä¹ å…³æ³¨æºæ–‡")
        
        # å¯è§†åŒ–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ attentionï¼ˆæ–‡æœ¬å½¢å¼ï¼‰
        print(f"\n   ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ Cross-Attention (head 0, ç¬¬ä¸€ä¸ªtgtä½ç½®å…³æ³¨srcå„ä½ç½®):")
        attn_first = cross_attn_weights[0, 0, 0, :].cpu().numpy()  # (src_len,)
        for i, w in enumerate(attn_first[:min(10, len(attn_first))]):
            bar = "â–ˆ" * int(w * 50)
            print(f"     src[{i}]: {w:.4f} {bar}")
        if len(attn_first) > 10:
            print(f"     ... (å…± {len(attn_first)} ä¸ªä½ç½®)")


def diagnose_model(checkpoint_path, data_dir='./wmt14_data'):
    """è¯Šæ–­æ¨¡å‹çŠ¶æ€"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("=" * 60)
    print("ğŸ” æ¨¡å‹è¯Šæ–­")
    print("=" * 60)
    
    # 1. åŠ è½½æ¨¡å‹
    print("\n1ï¸âƒ£ åŠ è½½æ¨¡å‹...")
    model, src_tokenizer, tgt_tokenizer, vocab_info, device = load_model_and_tokenizers(
        checkpoint_path, data_dir, device
    )
    model.eval()
    print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"   è®¾å¤‡: {device}")
    print(f"   è¯æ±‡è¡¨å¤§å°: src={vocab_info.get('src_vocab_size', 'N/A')}, tgt={vocab_info.get('tgt_vocab_size', 'N/A')}")
    
    # æ‰“å°å…³é”® token IDs
    print(f"\n   ç‰¹æ®Š Token IDs:")
    print(f"     PAD: {vocab_info.get('pad_token_id')}")
    print(f"     BOS: {vocab_info.get('bos_token_id')}")
    print(f"     EOS: {vocab_info.get('eos_token_id')}")
    print(f"     src_pad: {vocab_info.get('src_pad_token_id')}")
    print(f"     tgt_pad: {vocab_info.get('tgt_pad_token_id')}")
    
    # 2. æ£€æŸ¥æƒé‡ç»Ÿè®¡
    print("\n2ï¸âƒ£ æ£€æŸ¥æƒé‡ç»Ÿè®¡...")
    for name, param in model.named_parameters():
        if 'weight' in name and param.dim() >= 2:
            mean = param.data.mean().item()
            std = param.data.std().item()
            min_val = param.data.min().item()
            max_val = param.data.max().item()
            
            # æ£€æŸ¥å¼‚å¸¸å€¼
            has_nan = torch.isnan(param.data).any().item()
            has_inf = torch.isinf(param.data).any().item()
            
            if has_nan or has_inf or std < 1e-6 or std > 10:
                status = "âš ï¸ å¼‚å¸¸"
            else:
                status = "âœ…"
            
            # åªæ‰“å°å…³é”®å±‚
            if 'attention' in name or 'embedding' in name or 'output_projection' in name:
                print(f"   {status} {name}: mean={mean:.4f}, std={std:.4f}, range=[{min_val:.4f}, {max_val:.4f}]")
    
    # 3. æµ‹è¯•å‰å‘ä¼ æ’­
    print("\n3ï¸âƒ£ æµ‹è¯•å‰å‘ä¼ æ’­...")
    test_text = "Hallo, wie geht es Ihnen?"  # ç®€å•å¾·è¯­å¥å­
    
    # ç¼–ç 
    src_enc = src_tokenizer.encode(test_text)
    src_ids = src_enc.ids
    eos_id = src_tokenizer.token_to_id('</s>')
    bos_id = tgt_tokenizer.token_to_id('<s>')
    pad_id = tgt_tokenizer.token_to_id('<pad>')
    
    src_ids = src_ids + [eos_id]
    src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)
    
    print(f"   æºæ–‡æœ¬: {test_text}")
    print(f"   æºtokens: {src_ids}")
    print(f"   æºé•¿åº¦: {len(src_ids)}")
    
    # æ£€æŸ¥ PAD token æ˜¯å¦æ­£ç¡®
    print(f"\n   PAD Token æ£€æŸ¥:")
    print(f"     pad_id = {pad_id}")
    print(f"     æºåºåˆ—ä¸­ PAD æ•°é‡: {sum(1 for t in src_ids if t == pad_id)}")
    print(f"     æ¨¡å‹çš„ src_pad_id: {model.src_pad_id}")
    print(f"     æ¨¡å‹çš„ tgt_pad_id: {model.tgt_pad_id}")
    
    if pad_id != model.src_pad_id:
        print(f"   âš ï¸ è­¦å‘Š: åˆ†è¯å™¨ PAD ID ({pad_id}) ä¸æ¨¡å‹ src_pad_id ({model.src_pad_id}) ä¸ä¸€è‡´ï¼")
    
    with torch.no_grad():
        # ç¼–ç å™¨è¾“å‡º
        encoder_output, src_mask = model.encode(src_tensor)
        print(f"   Encoderè¾“å‡ºå½¢çŠ¶: {encoder_output.shape}")
        print(f"   Encoderè¾“å‡ºç»Ÿè®¡: mean={encoder_output.mean().item():.4f}, std={encoder_output.std().item():.4f}")
        
        # æ£€æŸ¥encoderè¾“å‡ºæ˜¯å¦æœ‰æ•ˆ
        if encoder_output.std().item() < 1e-6:
            print("   âš ï¸ Encoderè¾“å‡ºæ–¹å·®è¿‡å°ï¼Œå¯èƒ½æœ‰é—®é¢˜ï¼")
        
        # è§£ç å™¨æµ‹è¯•ï¼ˆå•æ­¥ï¼‰
        tgt_start = torch.tensor([[bos_id]], dtype=torch.long, device=device)
        decoder_output = model.decode(tgt_start, encoder_output, src_mask)
        logits = model.output_projection(decoder_output)
        
        print(f"   Decoderè¾“å‡ºå½¢çŠ¶: {decoder_output.shape}")
        print(f"   Decoderè¾“å‡ºç»Ÿè®¡: mean={decoder_output.mean().item():.4f}, std={decoder_output.std().item():.4f}")
        print(f"   Logitså½¢çŠ¶: {logits.shape}")
        
        # æŸ¥çœ‹top-5é¢„æµ‹
        probs = torch.softmax(logits[0, 0], dim=-1)
        top_probs, top_ids = torch.topk(probs, 5)
        print(f"   Top-5é¢„æµ‹:")
        for i, (prob, idx) in enumerate(zip(top_probs.tolist(), top_ids.tolist())):
            token = tgt_tokenizer.decode([idx]) if hasattr(tgt_tokenizer, 'decode') else f"ID:{idx}"
            print(f"     {i+1}. '{token}' (prob={prob:.4f}, id={idx})")
    
    # 4. æ£€æŸ¥ Cross-Attentionï¼ˆå…³é”®è¯Šæ–­ï¼ï¼‰
    print("\n4ï¸âƒ£ Cross-Attention è¯Šæ–­ï¼ˆå…³é”®ï¼ï¼‰...")
    tgt_start = torch.tensor([[bos_id]], dtype=torch.long, device=device)
    check_cross_attention(model, src_tensor, tgt_start, device)
    
    # 5. ä½¿ç”¨greedyå’Œbeamåˆ†åˆ«ç”Ÿæˆ
    print("\n5ï¸âƒ£ æµ‹è¯•ç”Ÿæˆ...")
    from inference import TransformerInference
    
    inference = TransformerInference(
        model, src_tokenizer, tgt_tokenizer, device,
        vocab_info['bos_token_id'], vocab_info['eos_token_id'], vocab_info['pad_token_id']
    )
    
    # Greedy
    print("   Greedyè§£ç :")
    greedy_result = inference.greedy_decode(test_text, max_length=50)
    print(f"   è¾“å…¥: {test_text}")
    print(f"   è¾“å‡º: {greedy_result}")
    
    # Beam
    print("\n   Beam Searchè§£ç :")
    beam_result = inference.beam_search_decode(test_text, beam_size=4, max_length=50)
    print(f"   è¾“å…¥: {test_text}")
    print(f"   è¾“å‡º: {beam_result}")
    
    # 6. æµ‹è¯•æ›´é•¿çš„å¥å­
    print("\n6ï¸âƒ£ æµ‹è¯•WMTé£æ ¼å¥å­...")
    long_text = "Die EuropÃ¤ische Union hat neue Regeln fÃ¼r den Datenschutz eingefÃ¼hrt."
    
    greedy_long = inference.greedy_decode(long_text, max_length=100)
    print(f"   æºæ–‡: {long_text}")
    print(f"   Greedy: {greedy_long}")
    
    beam_long = inference.beam_search_decode(long_text, beam_size=4, max_length=100)
    print(f"   Beam:   {beam_long}")
    
    print("\n" + "=" * 60)
    print("ğŸ” è¯Šæ–­å®Œæˆ")
    print("=" * 60)


def check_tokenizer_ids(data_dir='./wmt14_data'):
    """æ£€æŸ¥åˆ†è¯å™¨çš„ç‰¹æ®Š token IDs æ˜¯å¦æ­£ç¡®"""
    print("\n" + "=" * 60)
    print("ğŸ” åˆ†è¯å™¨ Token ID æ£€æŸ¥")
    print("=" * 60)
    
    # å°è¯•åŠ è½½ SentencePiece
    spm_model = os.path.join(data_dir, 'spm_shared.model')
    if os.path.exists(spm_model):
        try:
            import sentencepiece as spm
            sp = spm.SentencePieceProcessor(model_file=spm_model)
            
            print("\nSentencePiece åˆ†è¯å™¨:")
            print(f"  è¯è¡¨å¤§å°: {sp.get_piece_size()}")
            print(f"\n  ç‰¹æ®Š Token IDs (å†…ç½®æ–¹æ³•):")
            print(f"    sp.pad_id() = {sp.pad_id()}")
            print(f"    sp.unk_id() = {sp.unk_id()}")
            print(f"    sp.bos_id() = {sp.bos_id()}")
            print(f"    sp.eos_id() = {sp.eos_id()}")
            
            print(f"\n  piece_to_id æ–¹æ³• (ä»£ç ä¸­ä½¿ç”¨çš„):")
            print(f"    piece_to_id('<pad>') = {sp.piece_to_id('<pad>')}")
            print(f"    piece_to_id('<unk>') = {sp.piece_to_id('<unk>')}")
            print(f"    piece_to_id('<s>') = {sp.piece_to_id('<s>')}")
            print(f"    piece_to_id('</s>') = {sp.piece_to_id('</s>')}")
            
            # å…³é”®æ£€æŸ¥ï¼
            if sp.pad_id() != sp.piece_to_id('<pad>'):
                print(f"\n  âš ï¸ ä¸¥é‡è­¦å‘Š: pad_id() != piece_to_id('<pad>')!")
                print(f"     è¿™ä¼šå¯¼è‡´ mask è®¡ç®—é”™è¯¯ï¼Œcross-attention ä¼šå¤±æ•ˆï¼")
            
            # æµ‹è¯•ç¼–ç 
            test_text = "Hello world"
            encoded = sp.encode(test_text, out_type=int)
            print(f"\n  æµ‹è¯•ç¼–ç  '{test_text}':")
            print(f"    IDs: {encoded}")
            print(f"    è§£ç : {sp.decode(encoded)}")
            
        except Exception as e:
            print(f"  åŠ è½½å¤±è´¥: {e}")
    else:
        print(f"  SentencePiece æ¨¡å‹ä¸å­˜åœ¨: {spm_model}")


def check_data_alignment(data_dir='./wmt14_data', num_samples=5):
    """æ£€æŸ¥æ•°æ®å¯¹é½æ˜¯å¦æ­£ç¡®"""
    print("\n" + "=" * 60)
    print("ğŸ” æ•°æ®å¯¹é½æ£€æŸ¥")
    print("=" * 60)
    
    de_path = os.path.join(data_dir, 'valid.de')
    en_path = os.path.join(data_dir, 'valid.en')
    
    if not os.path.exists(de_path) or not os.path.exists(en_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_dir}")
        return
    
    print(f"\nå‰{num_samples}ä¸ªéªŒè¯é›†æ ·æœ¬:")
    with open(de_path, 'r', encoding='utf-8') as f_de, \
         open(en_path, 'r', encoding='utf-8') as f_en:
        for i, (de_line, en_line) in enumerate(zip(f_de, f_en)):
            if i >= num_samples:
                break
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"  DE: {de_line.strip()[:100]}...")
            print(f"  EN: {en_line.strip()[:100]}...")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¨¡å‹è¯Šæ–­')
    parser.add_argument('--checkpoint', default='/root/autodl-tmp/checkpoints/best_model.pt', 
                        help='æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data-dir', default='./wmt14_data', help='æ•°æ®ç›®å½•')
    parser.add_argument('--check-data', action='store_true', help='æ£€æŸ¥æ•°æ®å¯¹é½')
    
    args = parser.parse_args()
    
    # é¦–å…ˆæ£€æŸ¥åˆ†è¯å™¨çš„ token IDsï¼ˆè¿™æ˜¯æœ€å¯èƒ½å‡ºé—®é¢˜çš„åœ°æ–¹ï¼ï¼‰
    check_tokenizer_ids(args.data_dir)
    
    if args.check_data:
        check_data_alignment(args.data_dir)
    
    if os.path.exists(args.checkpoint):
        diagnose_model(args.checkpoint, args.data_dir)
    else:
        print(f"âŒ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {args.checkpoint}")
        print("è¯·ä½¿ç”¨ --checkpoint æŒ‡å®šæ­£ç¡®çš„è·¯å¾„")
