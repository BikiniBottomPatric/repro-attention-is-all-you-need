"""
é…ç½®æ–‡ä»¶ - ç®€æ´æ¸…æ™°çš„å‚æ•°é…ç½®
æä¾›ä¸‰ç§é¢„è®¾é…ç½®ï¼Œå‚æ•°å«ä¹‰æ˜ç¡®
"""

# åŸºç¡€é…ç½® - ä¸¥æ ¼æŒ‰ç…§åŸè®ºæ–‡"Attention Is All You Need"è®¾ç½®
# é’ˆå¯¹ RTX Pro 6000 96GB ä¼˜åŒ–
CONFIG = {
    # æ•°æ®é…ç½® - ä¸¥æ ¼æŒ‰åŸè®ºæ–‡5.1èŠ‚: ~25K source + 25K target â‰ˆ 50K tokens/batch
    # çº¯ç›¸å¯¹è·¯å¾„ï¼šå‡è®¾ä»£ç å’Œæ•°æ®åœ¨åŒä¸€ç›®å½•ä¸‹
    # 'data_dir': './wmt14_data',  
    'data_dir': './multi30k_data', # æœ¬åœ°éªŒè¯ç”¨å°å‹æ•°æ®é›†
    
    # æ‰¹å¤§å°è§£é‡Šï¼š
    # è®ºæ–‡å®šä¹‰: "Each batch contained ~25,000 source tokens and ~25,000 target tokens"
    # ä½ çš„ TokenBatchSampler å·²ç»é™åˆ¶å•æ‰¹ ~25k tokensï¼Œç¬¦åˆè®ºæ–‡è®¾ç½®
    # æ¢¯åº¦ç´¯ç§¯å¯é€‰ï¼šaccumulate=1 å·²åŒ¹é…è®ºæ–‡å•æ‰¹å¤§å°
    # å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼Œå¯ä»¥å°è¯• accumulate=2 æˆ– 4 å¢åŠ æœ‰æ•ˆ batch
    # 'max_tokens_per_batch': 25000,
    'max_tokens_per_batch': 4000,  # 96GB VRAM ä¼˜åŒ– (FP32)
    'max_sentences_per_batch': None,  # å¯é€‰ï¼šæ¯æ‰¹æœ€å¤§å¥å­æ•°ä¸Šé™ï¼ˆä¿é™©é˜ˆå€¼ï¼‰
    'accumulate_grad_batches': 6,   # æ— éœ€ç´¯ç§¯ï¼Œç‰©ç† Batch ç›´æ¥å¯¹é½è®ºæ–‡æ ‡å‡†
    
    'max_src_len': 256,  # åˆç†çš„åºåˆ—é•¿åº¦ï¼Œå¹³è¡¡å†…å­˜å’Œæ€§èƒ½
    'max_tgt_len': 256,  # åˆç†çš„åºåˆ—é•¿åº¦ï¼Œå¹³è¡¡å†…å­˜å’Œæ€§èƒ½
    
    # æ¨¡å‹é…ç½® - ä¸¥æ ¼æŒ‰è®ºæ–‡Baseæ¨¡å‹ (Table 3)
    'd_model': 512,      # è®ºæ–‡Table 3: Base model
    'num_heads': 8,      # è®ºæ–‡Table 3: h=8
    'num_encoder_layers': 6,  # è®ºæ–‡Table 3: N=6 
    'num_decoder_layers': 6,
    'd_ff': 2048,        # è®ºæ–‡Table 3: d_ff=2048
    'dropout': 0.1,      # è®ºæ–‡5.4èŠ‚: P_drop=0.1
    
    # è®­ç»ƒé…ç½® - ä¸¥æ ¼æŒ‰è®ºæ–‡è®¾ç½®
    'warmup_steps': 4000,     # è®ºæ–‡5.3èŠ‚: warmup_steps=4000
    'lr_scale': 1.0,          # è®ºæ–‡å…¬å¼éšå«å€¼ä¸º1.0 (è™½ç„¶ç¤¾åŒºå¸¸ç”¨2.0åŠ é€Ÿï¼Œä½†ä¸ºäº†ä¸¥æ ¼å¤ç°æ”¹å›1.0)
    'label_smoothing': 0.1,   # è®ºæ–‡5.4èŠ‚: Îµ_ls=0.1
    'num_epochs': 30,         # Multi30k éªŒè¯ç”¨ 30 è½®å³å¯
    # 'num_epochs': 45,         # åŸè®ºæ–‡100Kæ­¥: 4.5MÃ—25tokens / 50K batch â‰ˆ 2250 steps/epoch, éœ€~45 epochs
    # æ³¨æ„ï¼šå¦‚æœBatchSizeå‡åŠäº†ï¼ŒStepæ•°ä¼šç¿»å€ï¼Œè¿™é‡ŒEpochæ•°ä¿æŒä¸å˜ï¼Œæ€»Stepæ•°ä¼šè‡ªåŠ¨ç¿»å€åŒ¹é…
    
    # éªŒè¯ç­–ç•¥ï¼šå®Œæ•´éªŒè¯é›†ï¼ˆæä¾›ç¨³å®šå‡†ç¡®çš„æŒ‡æ ‡ï¼‰
    # éªŒè¯åªåœ¨æ¯ä¸ªepochç»“æŸæ—¶è¿›è¡Œï¼Œä½¿ç”¨å®Œæ•´éªŒè¯é›†
    # è¿™æ ·èƒ½é¿å…éƒ¨åˆ†æ•°æ®å¯¼è‡´çš„æ³¢åŠ¨ï¼Œæ›´çœŸå®åæ˜ æ¨¡å‹æ”¶æ•›æƒ…å†µ
    
    # ä¿å­˜é…ç½®
    # æ”¹ä¸ºç›¸å¯¹è·¯å¾„ï¼Œé€‚åº”ä¸åŒç¯å¢ƒ
    'save_dir': './checkpoints_multi30k',
    'save_interval': 1,  # æ¯ä¸ªepochä¿å­˜ä¸€æ¬¡ï¼ˆbest_model.ptå§‹ç»ˆä¿å­˜ï¼‰
    
    # è¯„ä¼°é…ç½®ï¼ˆæœ€ç»ˆBLEUè¯„ä¼°ä½¿ç”¨ï¼‰
    'eval_batch_size': 32,     # æ¨ç†æ‰¹å¤„ç†å¤§å°ï¼ˆevaluate.pyä½¿ç”¨ï¼‰
    'eval_method': 'greedy',   # è®­ç»ƒæ—¶ç”¨greedyè¯„ä¼°ï¼ˆæ›´å¿«ï¼Œä¾¿äºè¯Šæ–­ï¼‰
    'eval_beam_size': 4,       # æŸæœç´¢å¤§å° (tensor2tensorå®˜æ–¹å®ç°ä¾æ®)
    'eval_max_length': 100,    # æ¢å¤æ­£å¸¸æœ€å¤§é•¿åº¦
    'eval_length_penalty': 0.6,   # æ¢å¤è®ºæ–‡è®¾ç½®
    'eval_bleu_per_epoch': True,   # æ˜¯å¦åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¿›è¡ŒBLEUè¯„ä¼°
    'eval_bleu_every_n_epochs': 1, # æ¯å¤šå°‘ä¸ªepochè®¡ç®—ä¸€æ¬¡BLEUï¼ˆ1=æ¯ä¸ªepochï¼‰
    # æ¨ç†é…ç½® - ä¸¥æ ¼æŒ‰åŸè®ºæ–‡ï¼ˆçº¯ beam search + length penaltyï¼Œæ— é¢å¤– trickï¼‰
    # 'no_repeat_ngram_size': 0,     # âŒ å…³é—­ n-gram é‡å¤æŠ‘åˆ¶ï¼ˆåŸè®ºæ–‡æ— æ­¤è®¾ç½®ï¼‰
    # 'min_decode_length': 0,        # âŒ å…³é—­æœ€å°ç”Ÿæˆé•¿åº¦ï¼ˆåŸè®ºæ–‡æ— æ­¤è®¾ç½®ï¼‰
    # 'eos_bias': 0.0,               # âŒ å…³é—­ EOS åç½®ï¼ˆåŸè®ºæ–‡æ— æ­¤è®¾ç½®ï¼‰
    # 'repetition_penalty': 1.0,     # âŒ å…³é—­é‡å¤æƒ©ç½šï¼ˆåŸè®ºæ–‡æ— æ­¤è®¾ç½®ï¼‰
    
    # è¯æ±‡é…ç½®
    'vocab_size': 10000,       # è®ºæ–‡æ ‡å‡†è¯æ±‡å¤§å°
    # 'vocab_size': 10000,       # Multi30k å°å‹è¯è¡¨
    'vocab_mode': 'shared',    # é»˜è®¤å…±äº«è¯è¡¨ï¼Œæ›´è´´è¿‘è®ºæ–‡ä¸ç¤¾åŒºå¸¸è§å®è·µ
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½® - é’ˆå¯¹ RTX Pro 6000 + 22æ ¸CPU ä¼˜åŒ–
    'num_workers': 16,         # é‡Šæ”¾ CPU æ€§èƒ½åŠ é€Ÿæ•°æ®åŠ è½½
    # 'use_compile': False,      # æš‚æ—¶å…³é—­ - torch.compileä¸maskæ“ä½œæœ‰å…¼å®¹æ€§é—®é¢˜
    # 'use_amp': False,          # å…³é—­æ··åˆç²¾åº¦è®­ç»ƒä»¥è¿˜åŸè®ºæ–‡è®¾ç½®ï¼Œé¿å…æ•°å€¼ä¸ç¨³å®šæ€§
    # 'compile_mode': 'reduce-overhead',  # å¹³è¡¡ç¼–è¯‘æ—¶é—´å’Œè¿è¡Œæ•ˆç‡
    'use_hf_data': True,       # ä½¿ç”¨datasets.mapåŠ¨æ€æ•°æ®ç®¡çº¿ï¼ˆæ— éœ€*.ptç¼“å­˜ï¼‰
    # æ•°æ®é›†é€‰æ‹©ï¼ˆå¯åˆ‡æ¢åˆ° iwslt2014 ç­‰ï¼‰
    'hf_dataset': 'wmt14',     # HF æ•°æ®é›†åç§°: wmt14 / iwslt2014 / ...
    'hf_subset': 'de-en',      # æ•°æ®é›†å­é…ç½®
    
    # æ•°æ®æ¸…æ´—ï¼ˆä¸ç¤¾åŒºå®è·µä¸€è‡´ï¼‰
    'drop_too_long': True,           # è®­ç»ƒé›†ä¸­ä¸¢å¼ƒè¿‡é•¿æ ·æœ¬ï¼ˆè€Œéä»…æˆªæ–­ï¼‰
    'length_ratio_threshold': 2.0,   # æ”¾æ¾é•¿åº¦æ¯”é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šæ ·æœ¬

    # åˆ†è¯å™¨åç«¯ï¼š
    # 'bpe' = HuggingFace Tokenizers ByteLevel BPEï¼ˆGPT-2/RoBERTaé£æ ¼ï¼‰
    # 'sentencepiece' = Google SentencePieceï¼ˆåŸè®ºæ–‡tensor2tensorä½¿ç”¨ï¼Œæ¨èè¿˜åŸï¼‰
    'tokenizer_backend': 'sentencepiece',
    # å¦‚éœ€å¼ºåˆ¶é‡è®­åˆ†è¯å™¨ï¼ˆæ¸…ç†æ—§æ–‡ä»¶åé‡è®­ï¼‰ï¼Œè®¾ç½®ä¸º True
    'tokenizer_force_retrain': True,  # åˆ‡æ¢åˆ°WMT14éœ€è¦é‡æ–°è®­ç»ƒåˆ†è¯å™¨
    'sp_vocab_size': 10000,
    'sp_model_type': 'bpe',  # bpe | unigramï¼ˆåŸè®ºæ–‡ä½¿ç”¨BPEï¼‰
    'sp_character_coverage': 1.0,
}

def get_config():
    """è·å–é…ç½®"""
    return CONFIG.copy()


def print_config():
    """æ‰“å°é…ç½®è¯¦æƒ…"""
    config = get_config()
    
    print(f"\nğŸ“‹ TRANSFORMER é…ç½®:")
    print("=" * 40)
    
    print("ğŸ“ æ•°æ®:")
    print(f"  æ•°æ®ç›®å½•: {config['data_dir']}")
    print(f"  æ¯æ‰¹æœ€å¤§tokens: {config['max_tokens_per_batch']}")
    if config.get('max_sentences_per_batch'):
        print(f"  æ¯æ‰¹æœ€å¤§å¥å­æ•°: {config['max_sentences_per_batch']}")
    print(f"  æ¢¯åº¦ç´¯ç§¯: {config['accumulate_grad_batches']} æ­¥")
    # ä¿®æ­£æ˜¾ç¤ºï¼šæœ‰æ•ˆtokensæ˜¯ accumulated * max_tokens (è¿™é‡Œè¿‘ä¼¼ä¸º src+tgt)
    # åŸå…ˆçš„è®¡ç®—æœ‰è¯¯å¯¼æ€§ï¼Œç°åœ¨æ˜ç¡®ä¸º Step Batch Size
    # æ³¨æ„ï¼šmax_tokens_per_batch é™åˆ¶çš„æ˜¯ max(src, tgt)ï¼Œæ‰€ä»¥å•æ‰¹å®é™…åŒ…å« src+tgt ~ 2*max_tokens (å¦‚æœå¡«æ»¡)
    # ä½†åŸè®ºæ–‡çš„ 25000 æŒ‡çš„æ˜¯ src å’Œ tgt å„ 25000ã€‚
    # æˆ‘ä»¬çš„ sampler é™åˆ¶ max(src, tgt) * batch <= 25000
    # æ‰€ä»¥å•æ‰¹ src <= 25000, tgt <= 25000. 
    # æ­£å¥½å¯¹åº”è®ºæ–‡çš„ä¸€ä¸ª Batchã€‚
    effective_tokens = "25k Src + 25k Tgt" if config['accumulate_grad_batches'] == 1 else f"{25 * config['accumulate_grad_batches']}k Src + {25 * config['accumulate_grad_batches']}k Tgt"
    
    print(f"  æœ‰æ•ˆæ‰¹å¤§å°: {effective_tokens} (åŒ¹é…åŸè®ºæ–‡: 25k+25k)" if config['accumulate_grad_batches'] == 1 else f"  æœ‰æ•ˆæ‰¹å¤§å°: {effective_tokens} (âš ï¸ å¤§äºåŸè®ºæ–‡)")
    print(f"  æœ€å¤§æºé•¿åº¦: {config['max_src_len']}")
    print(f"  æœ€å¤§ç›®æ ‡é•¿åº¦: {config['max_tgt_len']}")
    
    print("\nğŸ—ï¸ æ¨¡å‹:")
    print(f"  æ¨¡å‹ç»´åº¦: {config['d_model']}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {config['num_heads']}")
    print(f"  Encoderå±‚æ•°: {config['num_encoder_layers']}")
    print(f"  Decoderå±‚æ•°: {config['num_decoder_layers']}")
    print(f"  FFNç»´åº¦: {config['d_ff']}")
    print(f"  Dropout: {config['dropout']}")
    
    print("\nğŸš€ è®­ç»ƒ:")
    print(f"  Warmupæ­¥æ•°: {config['warmup_steps']}")
    print(f"  å­¦ä¹ ç‡ç¼©æ”¾: {config.get('lr_scale', 1.0)}")
    print(f"  æ ‡ç­¾å¹³æ»‘: {config['label_smoothing']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['num_epochs']}")
    print(f"  ä¿å­˜é—´éš”: {config['save_interval']} epochs")
    print(f"  ä¿å­˜ç›®å½•: {config['save_dir']}")
    print(f"  éªŒè¯ç­–ç•¥: å®Œæ•´éªŒè¯é›† (æ¯epochç»“æŸæ—¶è¿›è¡Œ)")
    
    print("\nğŸ“Š è¯„ä¼°:")
    print(f"  æ¨ç†æ‰¹å¤„ç†å¤§å°: {config['eval_batch_size']}")
    print(f"  è§£ç æ–¹æ³•: {config['eval_method']}")
    print(f"  æŸæœç´¢å¤§å°: {config['eval_beam_size']}")
    print(f"  æœ€å¤§ç”Ÿæˆé•¿åº¦: {config['eval_max_length']}")
    print(f"  æ¯epoch BLEUè¯„ä¼°: {'âœ…' if config['eval_bleu_per_epoch'] else 'âŒ'}")
    
    print("\nâš¡ æ€§èƒ½ä¼˜åŒ–:")
    print(f"  DataLoaderè¿›ç¨‹æ•°: {config['num_workers']}")
    # print(f"  torch.compile: {'âœ…' if config['use_compile'] else 'âŒ'} ({config.get('compile_mode', 'N/A')}æ¨¡å¼)")
    # print(f"  æ··åˆç²¾åº¦è®­ç»ƒ: {'âœ…' if config['use_amp'] else 'âŒ'} (èŠ‚çœæ˜¾å­˜)")
    print(f"  è¯æ±‡è¡¨å¤§å°: {config['vocab_size']}")
    
    # é¢„ä¼°å‚æ•°é‡
    params = estimate_params(config)
    print(f"\nğŸ”§ é¢„ä¼°:")
    print(f"  å‚æ•°æ•°é‡: ~{params:,}")
    fp32_size = params * 4 / 1024 / 1024
    fp16_size = params * 2 / 1024 / 1024
    print(f"  æ¨¡å‹å¤§å°(FP32): ~{fp32_size:.1f} MB")
    print(f"  æ¨¡å‹å¤§å°(FP16): ~{fp16_size:.1f} MB (AMPå¯ç”¨æ—¶)")
    print(f"  é¢„æœŸåŠ é€Ÿ: 2-3x (torch.compile + AMP)")


def estimate_params(config):
    """é¢„ä¼°æ¨¡å‹å‚æ•°æ•°é‡"""
    d_model = config['d_model']
    d_ff = config['d_ff']
    num_layers = config['num_encoder_layers'] + config['num_decoder_layers']
    vocab_size = config.get('vocab_size', 37000)  # ä»configè¯»å–ï¼Œé¿å…ç¡¬ç¼–ç 
    
    # å‚æ•°è®¡ç®—å¸¸é‡
    SRC_TGT_EMBEDDINGS = 2  # src + tgt ä¸¤ä¸ªembeddingå±‚
    ATTN_PROJECTIONS = 4    # Q, K, V, O å››ä¸ªæ³¨æ„åŠ›æŠ•å½±å±‚
    NORMS_PER_LAYER = 3     # æ¯å±‚å¹³å‡3ä¸ªLayerNorm (encoder:2, decoder:3)
    
    # åµŒå…¥å±‚å‚æ•°
    embedding_params = vocab_size * d_model * SRC_TGT_EMBEDDINGS
    
    # æ³¨æ„åŠ›å±‚å‚æ•°  
    attn_params_per_layer = ATTN_PROJECTIONS * d_model * d_model
    
    # FFNå‚æ•°
    ffn_params_per_layer = d_model * d_ff + d_ff * d_model
    
    # Layer Normå‚æ•°
    norm_params_per_layer = d_model * NORMS_PER_LAYER
    
    # æ€»å‚æ•°
    total_params = (embedding_params + 
                   num_layers * (attn_params_per_layer + ffn_params_per_layer + norm_params_per_layer) +
                   d_model * vocab_size)  # è¾“å‡ºæŠ•å½±
    return total_params

if __name__ == "__main__":
    print_config()
