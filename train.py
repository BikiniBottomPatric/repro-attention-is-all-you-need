"""
Transformerè®­ç»ƒè„šæœ¬ - å®Œæ•´å®ç°è®ºæ–‡è®­ç»ƒç­–ç•¥
æ‰‹å·¥å®ç°æ‰€æœ‰è®­ç»ƒç›¸å…³åŠŸèƒ½ï¼Œä¸ä½¿ç”¨é«˜çº§å°è£…

å®ç°åŠŸèƒ½ï¼š
âœ… Label SmoothingæŸå¤±å‡½æ•° (æ‰‹å·¥å®ç°)
âœ… Adamä¼˜åŒ–å™¨ + å­¦ä¹ ç‡è°ƒåº¦ (Warmupç­–ç•¥ï¼ŒæŒ‰è®ºæ–‡å…¬å¼)
âœ… æ¢¯åº¦è£å‰ª
âœ… å®Œæ•´çš„è®­ç»ƒ/éªŒè¯å¾ªç¯
âœ… æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½
"""

import os
import time
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
# import sacrebleu  # ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥
from torch.optim.lr_scheduler import LambdaLR

# å¯¼å…¥AMPç›¸å…³åŠŸèƒ½
# try:
#     # ä¼˜å…ˆä½¿ç”¨æ–°API (PyTorch 2.0+)
#     from torch.amp import autocast, GradScaler
#     HAS_AMP = True
#     USE_NEW_AMP_API = True
# except ImportError:
#     try:
#         # å›é€€åˆ°æ—§API
#         from torch.cuda.amp import autocast, GradScaler
#         HAS_AMP = True
#         USE_NEW_AMP_API = False
#     except ImportError:
#         HAS_AMP = False
#         USE_NEW_AMP_API = False

# å¯é€‰çš„è¿›åº¦æ¡æ”¯æŒ
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False

from model import create_model
from data import create_data_loaders
from config import get_config, print_config
 

"""è®­ç»ƒå™¨ä½¿ç”¨ LambdaLR å®ç° warmup + inverse sqrt å­¦ä¹ ç‡è°ƒåº¦ã€‚"""


class Trainer:
    """Transformerè®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("ğŸ”„ åŠ è½½æ•°æ®...")
        self.train_loader, self.val_loader, self.vocab_info = create_data_loaders(config)
        print("âœ… æ•°æ®åŠ è½½å™¨å°±ç»ª")
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ”„ åˆ›å»ºæ¨¡å‹...")
        model_config = {
            'src_vocab_size': self.vocab_info['src_vocab_size'],
            'tgt_vocab_size': self.vocab_info['tgt_vocab_size'],
            'd_model': config['d_model'],
            'num_heads': config['num_heads'],
            'num_encoder_layers': config['num_encoder_layers'],
            'num_decoder_layers': config['num_decoder_layers'],
            'd_ff': config['d_ff'],
            'dropout': config['dropout'],
            'pad_token_id': self.vocab_info['pad_token_id'],
            'src_pad_token_id': self.vocab_info.get('src_pad_token_id', self.vocab_info['pad_token_id']),
            'tgt_pad_token_id': self.vocab_info.get('tgt_pad_token_id', self.vocab_info['pad_token_id'])
        }
        
        self.model = create_model(model_config)
        self.model = self.model.to(self.device)
        print("âœ… æ¨¡å‹å°±ç»ª")
        
        # æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒï¼ˆéœ€åœ¨ä¼˜åŒ–å‰åˆå§‹åŒ–ï¼Œä¾›ä¼˜åŒ–å‡½æ•°è¯»å–ï¼‰
        # self.use_amp = config.get('use_amp', False) and HAS_AMP and torch.cuda.is_available()
        # self.use_new_amp_api = USE_NEW_AMP_API
        # if self.use_amp:
        #     # GradScaler åœ¨æ–°APIä¸‹æ— éœ€ä¼ å…¥è®¾å¤‡å‚æ•°
        #     self.scaler = GradScaler() if self.use_new_amp_api else GradScaler()
        # else:
        self.use_amp = False
        self.scaler = None
        
        # åº”ç”¨æ€§èƒ½ä¼˜åŒ–
        print("ğŸ”§ åº”ç”¨æ€§èƒ½è®¾ç½®...")
        self._apply_optimizations()
        print("âœ… æ€§èƒ½è®¾ç½®å®Œæˆ")
        
        # æ‰‹å·¥å®ç°Label SmoothingæŸå¤±å‡½æ•°
        self.label_smoothing = config['label_smoothing']
        self.pad_token_id = self.vocab_info.get('tgt_pad_token_id', self.vocab_info['pad_token_id'])
        self.vocab_size = self.vocab_info['tgt_vocab_size']

        # åº”ç”¨æ€§èƒ½ä¼˜åŒ–
        print("ğŸ”§ åº”ç”¨æ€§èƒ½è®¾ç½®...")
        self._apply_optimizations()
        print("âœ… æ€§èƒ½è®¾ç½®å®Œæˆ")
        
        # ä¼˜åŒ–å™¨ - ä¸¥æ ¼æŒ‰è®ºæ–‡5.3èŠ‚è®¾ç½®: Î²1=0.9, Î²2=0.98, Îµ=10^-9
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=1.0,  # å­¦ä¹ ç‡ç”±Warmupè°ƒåº¦å™¨æ§åˆ¶ï¼Œåˆå§‹å€¼ä¼šè¢«è¦†ç›–
            betas=(0.9, 0.98),    # è®ºæ–‡5.3èŠ‚: Î²1=0.9, Î²2=0.98  
            eps=1e-9,             # è®ºæ–‡5.3èŠ‚: Îµ=10^-9
            weight_decay=0.0      # è®ºæ–‡ä¸­æœªä½¿ç”¨æƒé‡è¡°å‡
        )

        # Label Smoothing - ä¸¥æ ¼å¤ç°ç‰ˆ (æ’é™¤ Padding çš„å½±å“)
        class LabelSmoothingLoss(nn.Module):
            def __init__(self, padding_idx, smoothing=0.1):
                super().__init__()
                self.padding_idx = padding_idx
                self.smoothing = smoothing
                
            def forward(self, pred, gold):
                # pred: (B*T, V), gold: (B*T)
                gold = gold.contiguous().view(-1)
                n_class = pred.size(1)
                
                # 1. åˆ›å»º One-hot åˆ†å¸ƒ
                one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)
                
                # 2. åº”ç”¨ Label Smoothing
                # å…¬å¼: (1 - Îµ) * one_hot + Îµ / (V - 1)
                # æ³¨æ„ï¼šæ­¤å¤„æš‚ä¸å¤„ç† Paddingï¼Œä¹‹åç»Ÿä¸€ mask
                one_hot = one_hot * (1 - self.smoothing) + (1 - one_hot) * self.smoothing / (n_class - 1)
                
                # 3. å…³é”®ä¿®æ­£ï¼šå¼ºåˆ¶å°† Padding ä½ç½®çš„æ¦‚ç‡ç½®é›¶
                # è¿™æ ·æ¨¡å‹ä¸ä¼šè¢«è®­ç»ƒå»é¢„æµ‹ "è¿™ä¸ªè¯æœ‰ Îµ/(V-1) çš„æ¦‚ç‡æ˜¯ Padding"
                # è™½ç„¶ Loss è®¡ç®—æ—¶ä¼š mask æ‰ gold=padding çš„è¡Œï¼Œ
                # ä½†å¯¹äº gold!=padding çš„è¡Œï¼Œå…¶ target distribution ä¸­ padding ä½ç½®å¿…é¡»ä¸º 0
                one_hot[:, self.padding_idx] = 0.0
                
                # 4. é‡æ–°å½’ä¸€åŒ– (å¯é€‰ï¼Œä½†æ¨è)
                # ç”±äºç½®é›¶äº† padding æ¦‚ç‡ï¼Œæ€»å’Œç•¥å°äº 1ï¼Œå¯ä»¥é‡æ–°å½’ä¸€åŒ–ï¼Œ
                # ä½†é€šå¸¸ç›´æ¥ä½¿ç”¨å³å¯ï¼Œå› ä¸º KL æ•£åº¦ä¸»è¦å…³æ³¨ç›¸å¯¹å€¼ï¼Œä¸” padding æ¦‚ç‡é€šå¸¸æå°
                # mask = torch.ones_like(one_hot)
                # mask[:, self.padding_idx] = 0
                # one_hot = one_hot / one_hot.sum(dim=1, keepdim=True)
                
                log_prb = F.log_softmax(pred, dim=1)
                
                non_pad_mask = gold.ne(self.padding_idx)
                loss = -(one_hot * log_prb).sum(dim=1)
                loss = loss.masked_select(non_pad_mask).mean()
                return loss
        
        self.criterion = LabelSmoothingLoss(self.pad_token_id, self.label_smoothing)
        # Validation æ—¶ä¸ç”¨ label smoothingï¼ˆå’Œ example ä¸€è‡´ï¼‰
        self.val_criterion = nn.CrossEntropyLoss(ignore_index=self.pad_token_id, reduction='mean')
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ (ä½¿ç”¨LambdaLRå®ç°warmup+inverse sqrt) - ä¸¥æ ¼éµå¾ªè®ºæ–‡å…¬å¼
        d_model = float(config['d_model'])
        warmup_steps = float(config['warmup_steps'])
        lr_scale = float(config.get('lr_scale', 1.0))
        
        def _lr_lambda(step: int) -> float:
            """è®ºæ–‡å…¬å¼: lrate = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))"""
            step = max(step, 1)  # é¿å…step=0
            
            # è®¡ç®—å­¦ä¹ ç‡å› å­
            arg1 = step ** -0.5  # step^(-0.5)
            arg2 = step * (warmup_steps ** -1.5)  # step * warmup_steps^(-1.5)
            
            return lr_scale * (d_model ** -0.5) * min(arg1, arg2)
        
        self.scheduler = LambdaLR(self.optimizer, lr_lambda=_lr_lambda)
        
        # è®­ç»ƒçŠ¶æ€
        self.step = 0
        self.epoch = 0
        self.best_loss = float('inf')
        
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„å¼•å‘æ··æ·†ï¼‰
        abs_save_dir = os.path.abspath(config['save_dir'])
        os.makedirs(abs_save_dir, exist_ok=True)
        self.config['save_dir'] = abs_save_dir
        print(f"ä¿å­˜ç›®å½•: {abs_save_dir}")
        print(f"å·¥ä½œç›®å½•: {os.getcwd()}")
        
        # è¯„ä¼°åœ¨ evaluate.py ä¸­ç»Ÿä¸€å¤„ç†
        try:
            self._sanity_source_conditioning()
        except Exception as e:
            print(f"âš ï¸ æ¡ä»¶åŒ–è¯Šæ–­å¤±è´¥: {e}")

    def _sanity_source_conditioning(self):
        batch = next(iter(self.val_loader))
        src_seq = batch['src_ids'].to(self.device)
        tgt_seq = batch['tgt_ids'].to(self.device)

        decoder_input = tgt_seq[:, :-1]
        target = tgt_seq[:, 1:]

        self.model.eval()
        with torch.no_grad():
            logits = self.model(src_seq, decoder_input)
            loss = self.criterion(
                logits.reshape(-1, logits.size(-1)),
                target.reshape(-1)
            ).item()

            perm = torch.randperm(src_seq.size(0), device=src_seq.device)
            src_shuf = src_seq.index_select(0, perm)
            logits_shuf = self.model(src_shuf, decoder_input)
            loss_shuf = self.criterion(
                logits_shuf.reshape(-1, logits_shuf.size(-1)),
                target.reshape(-1)
            ).item()

        delta = loss_shuf - loss
        print(f"æ¡ä»¶åŒ–è¯Šæ–­: loss={loss:.4f}, shuffled_src_loss={loss_shuf:.4f}, delta={delta:+.4f}")
    
    def _apply_optimizations(self):
        """åº”ç”¨æ€§èƒ½ä¼˜åŒ–"""
        config = self.config
        
        # 1. torch.compileä¼˜åŒ– (PyTorch 2.0+)
        # use_compile = config.get('use_compile', False)
        # print(f"compile: {use_compile}")
        
        # if use_compile and hasattr(torch, 'compile'):
        #     compile_mode = config.get('compile_mode', 'default')
        #     print(f"torch.compile: {compile_mode}")
        #     try:
        #         self.model = torch.compile(self.model, mode=compile_mode)
        #     except Exception as e:
        #         print(f"compileå¤±è´¥: {e}")
        
        # 2. AMPæ··åˆç²¾åº¦è®­ç»ƒ
        # if self.use_amp:
        #     print(f"AMPå¯ç”¨ ({'amp' if self.use_new_amp_api else 'cuda.amp'})")
        
        # 3. æ˜¾ç¤ºå‚æ•°é‡
        self.total_params = sum(p.numel() for p in self.model.parameters())
        print(f"å‚æ•°é‡: {self.total_params:,}")
    
    def train_step(self, batch, accumulate_batches=1):
        """è®­ç»ƒå•ä¸ªbatch - æ”¯æŒæ¢¯åº¦ç´¯ç§¯å’ŒAMP"""
        self.model.train()
        
        # æ•°æ®ç§»åˆ°GPU
        src_seq = batch['src_ids'].to(self.device, non_blocking=True)
        tgt_seq = batch['tgt_ids'].to(self.device, non_blocking=True)
        
        # å‡†å¤‡decoderè¾“å…¥å’Œç›®æ ‡ (æŒ‰è®ºæ–‡ï¼Œè¾“å…¥é”™ä½1ä¸ªtoken)
        decoder_input = tgt_seq[:, :-1]  # å»æ‰æœ€åä¸€ä¸ªtoken
        target = tgt_seq[:, 1:]          # å»æ‰ç¬¬ä¸€ä¸ªtoken
        
        # å‰å‘ä¼ æ’­ - æ”¯æŒAMP
        def _forward():
            logits = self.model(src_seq, decoder_input)
            return self.criterion(logits.reshape(-1, logits.size(-1)), target.reshape(-1))
        
        # if self.use_amp:
        #     device_type = 'cuda' if self.use_new_amp_api and self.device.type == 'cuda' else 'cuda'
        #     ctx = autocast(device_type) if self.use_new_amp_api else autocast()
        #     with ctx:
        #         loss = _forward()
        # else:
        loss = _forward()
        
        # åå‘ä¼ æ’­ - æ”¯æŒAMP
        # if self.use_amp:
        #     self.scaler.scale(loss / accumulate_batches).backward()
        # else:
        (loss / accumulate_batches).backward()
        
        return loss.item()

    def optimizer_step(self):
        """æ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°å’Œæ¢¯åº¦æ¸…é›¶ - æ”¯æŒAMP"""
        
        # DEBUG: æ£€æŸ¥å‚æ•°æ˜¯å¦æ›´æ–° (åœ¨æ¢¯åº¦æ¸…é›¶å‰æ£€æŸ¥)
        # æ¯100æ­¥æˆ–è€…æ˜¯æ¢¯åº¦å¼‚å¸¸æ—¶éƒ½æ‰“å°è¯¦ç»†ä¿¡æ¯
        if self.step % 100 == 0:
            total_norm = 0.0
            nan_count = 0
            zero_grad_count = 0
            total_params = 0
            
            for p in self.model.parameters():
                if p.grad is not None:
                    total_params += 1
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
                    if torch.isnan(param_norm):
                        nan_count += 1
                    if param_norm.item() == 0.0:
                        zero_grad_count += 1
                        
            total_norm = total_norm ** 0.5
            
            # åªæœ‰å½“æ¢¯åº¦èŒƒæ•°ç¡®å®ä¸º0ï¼Œæˆ–è€…æœ‰NaNæ—¶æ‰è­¦å‘Š
            if total_norm == 0.0 or math.isnan(total_norm) or nan_count > 0:
                print(f"âš ï¸ æ¢¯åº¦è¯Šæ–­ [Step {self.step}]:")
                print(f"  Total Norm: {total_norm}")
                print(f"  NaN Grads: {nan_count}/{total_params}")
                print(f"  Zero Grads: {zero_grad_count}/{total_params}")
                print(f"  Current LR: {self.optimizer.param_groups[0]['lr']:.2e}")
            elif self.step % 100 == 0:
                 # æ­£å¸¸æƒ…å†µä¸‹æ‰“å°ä¸€æ¬¡ Norm ç¡®è®¤æ•°å€¼èŒƒå›´
                 pass # print(f"  [Step {self.step}] Grad Norm: {total_norm:.4f}")
        
        # if self.use_amp:
        #     # AMPæ¨¡å¼ä¸‹çš„æ¢¯åº¦è£å‰ªå’Œä¼˜åŒ–å™¨æ›´æ–°
        #     # 1. Unscale gradients
        #     self.scaler.unscale_(self.optimizer)
        #     
        #     # 2. Clip gradients (now unscaled)
        #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        #     
        #     # 3. Update weights (scaler will skip if infs/NaNs are found)
        #     self.scaler.step(self.optimizer)
        #     
        #     # 4. Update scaler factor
        #     self.scaler.update()
        #     
        #     # 5. Zero grads
        #     self.optimizer.zero_grad()
        # else:
        # å¸¸è§„æ¨¡å¼ - æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()
        self.optimizer.zero_grad()
        
        # è°ƒç”¨scheduler.step()ï¼ˆPyTorch 1.1.0+è¦æ±‚åœ¨optimizer.step()ä¹‹åï¼‰
        # æ³¨æ„ï¼šé¦–æ¬¡è°ƒç”¨æ—¶PyTorchå¯èƒ½ä¼šå‘å‡ºè­¦å‘Šï¼Œè¿™æ˜¯é¢„æœŸè¡Œä¸ºï¼Œå¯ä»¥å¿½ç•¥
        self.scheduler.step()
        
        # å¢åŠ æ­¥æ•°è®¡æ•°ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        self.step += 1
        
        # è¯»å–å½“å‰å­¦ä¹ ç‡ç”¨äºæ—¥å¿—
        current_lr = self.optimizer.param_groups[0]['lr']
        return current_lr
    
    def validate(self):
        """å®Œæ•´éªŒè¯é›†éªŒè¯ - æä¾›ç¨³å®šå‡†ç¡®çš„lossã€perplexityä¸tokenå‡†ç¡®ç‡"""
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        total_samples = 0
        total_tokens = 0
        correct_tokens = 0
        
        print("éªŒè¯ä¸­...")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(self.val_loader):
                src_seq = batch['src_ids'].to(self.device)
                tgt_seq = batch['tgt_ids'].to(self.device)
                
                decoder_input = tgt_seq[:, :-1]
                target = tgt_seq[:, 1:]
                
                logits = self.model(src_seq, decoder_input)
                loss = self.val_criterion(
                    logits.reshape(-1, logits.size(-1)), 
                    target.reshape(-1)
                )
                
                total_loss += loss.item()
                num_batches += 1
                total_samples += src_seq.size(0)  # batch_size

                # Token-level accuracyï¼ˆå¿½ç•¥PADï¼‰
                pred = logits.argmax(dim=-1)  # (B, T-1)
                non_pad = target.ne(self.pad_token_id)
                if non_pad.any():
                    correct_tokens += int((pred.eq(target) & non_pad).sum().item())
                    total_tokens += int(non_pad.sum().item())
                
                # æ¯100ä¸ªbatchæ˜¾ç¤ºè¿›åº¦ï¼ˆå¯é€‰ï¼‰
                if (batch_idx + 1) % 100 == 0:
                    current_avg = total_loss / num_batches
                    current_ppl = math.exp(min(current_avg, 10))
                    current_acc = (100.0 * correct_tokens / total_tokens) if total_tokens > 0 else 0.0
                    print(f"  éªŒè¯è¿›åº¦: {batch_idx+1:,} æ‰¹æ¬¡, å½“å‰loss: {current_avg:.4f}, ppl: {current_ppl:.2f}, acc: {current_acc:.2f}%")
        
        avg_loss = total_loss / max(1, num_batches)
        perplexity = math.exp(min(avg_loss, 10))  # é˜²æ­¢æº¢å‡º
        token_acc = (100.0 * correct_tokens / total_tokens) if total_tokens > 0 else 0.0
        
        print(f"éªŒè¯å®Œæˆ: {num_batches:,} æ‰¹æ¬¡, {total_samples:,} æ ·æœ¬")
        
        return avg_loss, perplexity, token_acc
    
    def _ensure_tokenizers(self):
        """ç¡®ä¿åˆ†è¯å™¨å·²åŠ è½½ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if not hasattr(self, '_src_tokenizer'):
            from data import DataProcessor
            processor = DataProcessor(self.config['data_dir'])
            src_tok, tgt_tok = processor.load_tokenizers()
            if src_tok is None:
                raise FileNotFoundError("æœªæ‰¾åˆ°åˆ†è¯å™¨æ–‡ä»¶")
            self._src_tokenizer = src_tok
            self._tgt_tokenizer = tgt_tok
            self._eval_vocab_info = {
                'bos_token_id': tgt_tok.token_to_id('<s>'),
                'eos_token_id': tgt_tok.token_to_id('</s>'),
                'pad_token_id': tgt_tok.token_to_id('<pad>')
            }
    
    def evaluate_bleu(self, split='valid'):
        """BLEUè¯„ä¼°"""
        try:
            from evaluate import evaluate_bleu
            self._ensure_tokenizers()
            return evaluate_bleu(
                self.model, self._src_tokenizer, self._tgt_tokenizer,
                self._eval_vocab_info, self.device, self.config['data_dir'],
                batch_size=self.config.get('eval_batch_size', 32),
                method=self.config.get('eval_method', 'beam'),
                beam_size=self.config.get('eval_beam_size', 4),
                max_length=self.config.get('eval_max_length', 100),
                length_penalty=self.config.get('eval_length_penalty', 0.6),
                split=split
            )
        except Exception as e:
            print(f"BLEUè¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def save_checkpoint(self, filepath, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'step': self.step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_loss': self.best_loss,
            'config': self.config,
            'vocab_info': self.vocab_info
        }
        
        torch.save(checkpoint, filepath)
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
        
        if is_best:
            best_path = os.path.join(os.path.dirname(filepath), 'best_model.pt')
            torch.save(checkpoint, best_path)
            print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")
    
    def load_checkpoint(self, filepath):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        # å…¼å®¹æ–°æ—§è°ƒåº¦å™¨å­˜æ¡£
        if 'scheduler_state_dict' in checkpoint:
            try:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            except Exception:
                pass
        elif 'scheduler_step' in checkpoint:
            try:
                # å›é€€ï¼šä»…è®¾ç½®last_epochç”¨äºè¿‘ä¼¼æ¢å¤
                self.scheduler.last_epoch = int(checkpoint['scheduler_step'])
            except Exception:
                pass
        self.epoch = checkpoint['epoch']
        self.step = checkpoint['step']
        self.best_loss = checkpoint['best_loss']
        
        print(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}")
        print(f"æ¢å¤åˆ° epoch {self.epoch}, step {self.step}")
    
    def train(self, num_epochs, resume_from=None):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹å‚æ•°: {self.total_params:,}")
        print(f"è®­ç»ƒæ ·æœ¬: {len(self.train_loader.dataset):,}")
        print(f"éªŒè¯æ ·æœ¬: {len(self.val_loader.dataset):,}")
        
        # æ¢å¤è®­ç»ƒ
        if resume_from and os.path.exists(resume_from):
            self.load_checkpoint(resume_from)
        
        for epoch in range(self.epoch, num_epochs):
            self.epoch = epoch
            print(f"\n--- Epoch {epoch + 1}/{num_epochs} ---")
            
            # è®­ç»ƒ - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
            epoch_start_time = time.time()
            total_loss = 0
            num_batches = 0
            accumulate_batches = self.config['accumulate_grad_batches']
            
            # æ—¶é—´ä¼°ç®—å˜é‡ï¼ˆç®€åŒ–ï¼‰
            
            iterator = self.train_loader
            if HAS_TQDM:
                iterator = tqdm(self.train_loader, total=len(self.train_loader), desc=f"Epoch {epoch + 1}")
            
            for batch_idx, batch in enumerate(iterator):
                # è®¡ç®—å½“å‰ç´¯ç§¯æ­¥éª¤ä¸­å®é™…çš„batchæ•°é‡
                batches_in_current_step = (batch_idx % accumulate_batches) + 1
                is_last_batch = batch_idx == len(self.train_loader) - 1
                
                # å¦‚æœæ˜¯æœ€åä¸€ä¸ªbatchä¸”ä¸èƒ½æ•´é™¤ï¼Œä½¿ç”¨å®é™…çš„batchæ•°é‡
                if is_last_batch and (batch_idx + 1) % accumulate_batches != 0:
                    actual_accumulate = batches_in_current_step
                else:
                    actual_accumulate = accumulate_batches
                
                # è®­ç»ƒæ­¥éª¤ (ä½¿ç”¨æ­£ç¡®çš„ç´¯ç§¯æ•°é‡è¿›è¡Œæ¢¯åº¦ç¼©æ”¾)
                loss = self.train_step(batch, actual_accumulate)
                total_loss += loss
                num_batches += 1
                
                # å‰å‡ ä¸ªbatchç«‹å³æ˜¾ç¤ºè¿›åº¦ï¼ˆæ— tqdmæ—¶ï¼‰
                if not HAS_TQDM and batch_idx < 5:
                    print(f"  Batch {batch_idx+1}: Loss {loss:.4f} (ç´¯ç§¯: {batches_in_current_step}/{actual_accumulate})")
                
                # æ¯accumulate_batchesä¸ªbatchæ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°
                # æ³¨æ„ï¼šåªåœ¨å®Œæ•´çš„ç´¯ç§¯å‘¨æœŸç»“æŸæ—¶æ›´æ–°ï¼Œç¡®ä¿æ¢¯åº¦ç´¯ç§¯æ­£ç¡®
                if (batch_idx + 1) % accumulate_batches == 0:
                    lr = self.optimizer_step()
                    effective_step = (batch_idx + 1) // accumulate_batches  # æ­£ç¡®çš„æ­¥æ•°è®¡ç®—
                    
                    if HAS_TQDM:
                        avg_loss = total_loss / max(1, num_batches)
                        iterator.set_postfix(step=effective_step, loss=f"{loss:.4f}", avg=f"{avg_loss:.4f}", lr=f"{lr:.2e}")
                    else:
                        # ç®€åŒ–çš„è¿›åº¦æ˜¾ç¤º
                        if effective_step % 100 == 0 or effective_step <= 10:
                            avg_loss = total_loss / num_batches
                            progress = batch_idx + 1
                            total_batches = len(self.train_loader)
                            percent = 100 * progress / total_batches
                            
                            print(f"  Step {effective_step:>4} ({percent:5.1f}%) | "
                                  f"Loss: {loss:.4f} | Avg: {avg_loss:.4f} | LR: {lr:.2e}")
                    
                    # æ³¨æ„: éªŒè¯åªåœ¨epochç»“æŸæ—¶è¿›è¡Œï¼ˆå®Œæ•´éªŒè¯é›†ï¼‰
                    # è¿™æ ·èƒ½æä¾›æ›´ç¨³å®šå‡†ç¡®çš„losså’ŒperplexityæŒ‡æ ‡
                
                # æ¯1000ä¸ªbatchæ˜¾ç¤ºè¿›åº¦ (é¿å…åˆ·å±)
                elif not HAS_TQDM and batch_idx % 1000 == 0 and batch_idx > 0:
                    progress = batch_idx + 1
                    total_batches = len(self.train_loader)
                    percent = 100 * progress / total_batches
                    avg_loss = total_loss / num_batches
                    print(f"  è¿›åº¦: {percent:5.1f}% ({progress:>6}/{total_batches}) | Avg Loss: {avg_loss:.4f}")
            
            # å¤„ç†epochç»“æŸæ—¶å‰©ä½™çš„æ¢¯åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
            if (batch_idx + 1) % accumulate_batches != 0:
                # æœ€åä¸€æ‰¹ä¸è¶³accumulate_batchesï¼Œä½†ä»éœ€è¦æ›´æ–°
                lr = self.optimizer_step()
                effective_step = (batch_idx + 1 + accumulate_batches - 1) // accumulate_batches  # å‘ä¸Šå–æ•´
                print(f"  æœ€åä¸€æ‰¹æ¢¯åº¦æ›´æ–°: Step {effective_step} | LR: {lr:.2e}")
            
            train_loss = total_loss / num_batches
            epoch_time = time.time() - epoch_start_time
            
            # å®Œæ•´éªŒè¯é›†éªŒè¯ - æä¾›ç¨³å®šå‡†ç¡®çš„losså’Œperplexity
            val_loss, perplexity, val_acc = self.validate()
            
            # æ¯ä¸ªepochç»“æŸæ—¶çš„BLEUè¯„ä¼°
            epoch_bleu = None
            if self.config.get('eval_bleu_per_epoch', True):
                every_n = max(1, int(self.config.get('eval_bleu_every_n_epochs', 1)))
                if ((epoch + 1) % every_n) == 0:
                    print("è¯„ä¼°BLEU...")
                    epoch_bleu = self.evaluate_bleu('valid')
            
            print(f"Epoch {epoch + 1} å®Œæˆ ({epoch_time:.1f}s)")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
            print(f"  å›°æƒ‘åº¦: {perplexity:.2f}")
            print(f"  Tokenå‡†ç¡®ç‡: {val_acc:.2f}%")
            if epoch_bleu is not None:
                print(f"  BLEUåˆ†æ•°: {epoch_bleu:.2f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_loss < self.best_loss
            if is_best:
                self.best_loss = val_loss
                print(f"æœ€ä½³æ¨¡å‹æ›´æ–° (loss: {val_loss:.4f})")
                # ä»…ä¿å­˜å•ä¸€bestæ–‡ä»¶ï¼Œé¿å…é‡å¤å ç”¨ç£ç›˜
                best_ckpt_path = os.path.join(self.config['save_dir'], 'best_model.pt')
                self.save_checkpoint(best_ckpt_path, is_best=True)
            
            # å®šæœŸä¿å­˜ - å‡å°‘ä¿å­˜é¢‘ç‡ï¼Œæé«˜è®­ç»ƒæ•ˆç‡
            if (epoch + 1) % self.config['save_interval'] == 0:
                checkpoint_path = os.path.join(
                    self.config['save_dir'], f'checkpoint_epoch_{epoch + 1}.pt'
                )
                self.save_checkpoint(checkpoint_path, is_best)
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.4f}")
        
        # æœ€ç»ˆBLEUè¯„ä¼°ï¼ˆæµ‹è¯•é›†ï¼‰
        print("\nğŸ“Š è¿›è¡Œæœ€ç»ˆBLEUè¯„ä¼° (æµ‹è¯•é›†)...")
        final_bleu = self.evaluate_bleu('test')
        if final_bleu > 0:
            print(f"æœ€ç»ˆBLEUåˆ†æ•°: {final_bleu:.2f}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Transformerè®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--resume', default=None, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    
    args = parser.parse_args()
    
    # è·å–é…ç½®
    config = get_config()
    print_config()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    epochs = args.epochs if args.epochs is not None else config.get('num_epochs', 5)
    trainer.train(epochs, args.resume)


if __name__ == "__main__":
    main()
