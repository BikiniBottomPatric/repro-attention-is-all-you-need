"""
æ•°æ®å¤„ç† - ä½¿ç”¨ HuggingFace datasets çš„ç®€æ´ç®¡çº¿
åŠ è½½WMT14ã€åŠ è½½åˆ†è¯å™¨ã€æ‰¹é‡‡æ ·ä¸åŠ¨æ€è£å‰ª

æ³¨æ„ï¼šåˆ†è¯å™¨è®­ç»ƒå·²è§£è€¦åˆ° preprocess.pyï¼Œè¯·å…ˆè¿è¡Œ preprocess.py å†è¿è¡Œ train.py
"""

import os
import random
from typing import Optional, List, Dict, Any

import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

from tokenizers import Tokenizer
from tokenizers import decoders

# SentencePieceï¼ˆå¯é€‰ï¼‰
try:
    import sentencepiece as spm  # type: ignore
    HAS_SPM = True
except Exception:
    HAS_SPM = False

# å¯¼å…¥å…±äº«çš„ SPWrapper ç±»
from utils import SPWrapper
 

try:
    import datasets
    HAS_DATASETS = True
except ImportError:
    HAS_DATASETS = False
    print("âš ï¸ å»ºè®®å®‰è£… datasets: pip install datasets")


class Collator:
    """æ‰¹å¤„ç†å‡½æ•° - åŠ¨æ€æŒ‰batchå®é™…é•¿åº¦è£å‰ªï¼Œé¿å…å›ºå®špaddingå¼•å‘OOM"""

    def __init__(self, src_pad_token_id: int, tgt_pad_token_id: int):
        self.src_pad_token_id = src_pad_token_id
        self.tgt_pad_token_id = tgt_pad_token_id

    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        # å…ˆæŒ‰batchå†…æœ€å¤§é•¿åº¦è¿›è¡Œpaddingï¼Œå†è¿›è¡Œstack
        src_list = [item['src_ids'] if isinstance(item['src_ids'], torch.Tensor) else torch.tensor(item['src_ids'], dtype=torch.long)
                    for item in batch]
        tgt_list = [item['tgt_ids'] if isinstance(item['tgt_ids'], torch.Tensor) else torch.tensor(item['tgt_ids'], dtype=torch.long)
                    for item in batch]

        src_batch = pad_sequence(src_list, batch_first=True, padding_value=self.src_pad_token_id)  # (B, S_max)
        tgt_batch = pad_sequence(tgt_list, batch_first=True, padding_value=self.tgt_pad_token_id)  # (B, T_max)

        # åŠ¨æ€è£å‰ªåˆ°æœ¬batchå†…çš„æœ€å¤§épadé•¿åº¦
        with torch.no_grad():
            # æºä¾§æœ€å¤§æœ‰æ•ˆé•¿åº¦
            src_valid = (src_batch != self.src_pad_token_id)
            if src_valid.any():
                src_lengths = src_valid.sum(dim=1)
                src_max_len = int(src_lengths.max().item())
            else:
                src_max_len = src_batch.size(1)

            # ç›®æ ‡ä¾§æœ€å¤§æœ‰æ•ˆé•¿åº¦
            tgt_valid = (tgt_batch != self.tgt_pad_token_id)
            if tgt_valid.any():
                tgt_lengths = tgt_valid.sum(dim=1)
                tgt_max_len = int(tgt_lengths.max().item())
            else:
                tgt_max_len = tgt_batch.size(1)

        # åˆ‡åˆ°æœ€å°å¿…è¦é•¿åº¦ï¼ˆè‡³å°‘1ï¼Œé¿å…ç©ºå¼ é‡ï¼‰
        src_max_len = max(1, src_max_len)
        tgt_max_len = max(2, tgt_max_len)  # ç›®æ ‡è‡³å°‘ä¿ç•™2ä»¥ä¾¿åç»­shift

        src_batch = src_batch[:, :src_max_len].contiguous()
        tgt_batch = tgt_batch[:, :tgt_max_len].contiguous()

        return {
            'src_ids': src_batch,
            'tgt_ids': tgt_batch,
        }


class TokenBatchSampler:
    """ç®€æ´çš„æŒ‰ token æ•°æ‰“åŒ…é‡‡æ ·å™¨ï¼ˆå•æ¬¡é•¿åº¦è®¡ç®— + ç®€å•æ’åºï¼‰ã€‚"""

    def __init__(
        self,
        dataset: Dataset,
        max_tokens_per_batch: int,
        max_sentences_per_batch: Optional[int] = None,
        shuffle: bool = True,
    ):
        self.dataset = dataset
        self.max_tokens = int(max_tokens_per_batch)
        self.max_sentences = int(max_sentences_per_batch) if max_sentences_per_batch else None
        self.shuffle = shuffle
        
        # ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨é¢„è®¡ç®—çš„ 'length' åˆ—ï¼Œé¿å…é€è¡Œè¯»å–å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆ
        if hasattr(dataset, 'column_names') and 'length' in dataset.column_names:
            # å¦‚æœå­˜åœ¨ length åˆ—ï¼Œç›´æ¥è¯»å–ï¼ˆéå¸¸å¿«ï¼‰
            self.lengths = dataset['length']
            if isinstance(self.lengths, torch.Tensor):
                self.lengths = self.lengths.tolist()
        else:
            # å›é€€åˆ°æ—§æ–¹æ³•ï¼ˆè¾ƒæ…¢ï¼‰
            print("âš ï¸ è­¦å‘Š: æ•°æ®é›†ç¼ºå°‘ 'length' åˆ—ï¼Œæ­£åœ¨é€è¡Œè®¡ç®—é•¿åº¦ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰...")
            def item_len(i: int) -> int:
                ex = dataset[i]
                src_len = int(len(ex['src_ids']))
                tgt_len = int(len(ex['tgt_ids']))
                return max(1, max(src_len, tgt_len))
            self.lengths: List[int] = [item_len(i) for i in range(len(dataset))]
            
        self._build_order()

    def _build_order(self) -> None:
        idxs = list(range(len(self.dataset)))
        if self.shuffle:
            random.shuffle(idxs)
        # ç®€æ´ï¼šç›´æ¥æŒ‰é•¿åº¦æ’åºï¼Œå‡å°‘padding
        self.ordered = sorted(idxs, key=self.lengths.__getitem__)

    def __iter__(self):
        if self.shuffle:
            self._build_order()
        
        # 1. å…ˆç”Ÿæˆæ‰€æœ‰ batch
        batches = []
        batch: List[int] = []
        max_len = 0
        
        for i in self.ordered:
            l = self.lengths[i]
            new_max = l if l > max_len else max_len
            new_cnt = len(batch) + 1
            if (new_max * new_cnt <= self.max_tokens) and (self.max_sentences is None or new_cnt <= self.max_sentences):
                batch.append(i)
                max_len = new_max
            else:
                if batch:
                    batches.append(batch)
                batch = [i]
                max_len = l
        if batch:
            batches.append(batch)
            
        # 2. å…³é”®ä¿®å¤ï¼šæ‰“ä¹± batch çš„é¡ºåºï¼
        # ä¹‹å‰çš„ä»£ç è™½ç„¶æ‰“ä¹±äº†æ ·æœ¬ï¼Œä½†æ˜¯ sorted(key=len) ååˆå˜å›äº†æŒ‰é•¿åº¦ä¸¥æ ¼æ’åº
        # å¯¼è‡´æ¯ä¸ª epoch éƒ½æ˜¯ä»çŸ­å¥å­è®­ç»ƒåˆ°é•¿å¥å­ (Curriculum Learning)ï¼Œè¿™ä¼šä¸¥é‡é˜»ç¢æ”¶æ•›
        if self.shuffle:
            random.shuffle(batches)
            # åŒæ—¶æ‰“ä¹±æ¯ä¸ª batch å†…çš„æ ·æœ¬é¡ºåºï¼ˆæ›´å½»åº•çš„éšæœºåŒ–ï¼‰
            for batch in batches:
                random.shuffle(batch)
            
        # 3. Yield batches
        yield from batches

    def __len__(self) -> int:
        # ç²—ç•¥ä¼°è®¡ï¼šå¹³å‡é•¿åº¦ä¼°ç®—æ‰¹æ¬¡æ•°
        avg = sum(self.lengths) / max(1, len(self.lengths))
        per = max(1, int(self.max_tokens // max(1, avg)))
        return (len(self.lengths) + per - 1) // per


def create_data_loaders(config: Dict[str, Any]):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨
    - å½“ use_hf_data=True æ—¶ï¼šä½¿ç”¨ datasets.map åŠ¨æ€ç®¡çº¿ï¼ˆæ— éœ€ *.pt é¢„å¤„ç†æ–‡ä»¶ï¼‰
    - å¦åˆ™ï¼šæ²¿ç”¨å·²æœ‰ TensorDataset ç¼“å­˜æ–‡ä»¶
    """
    data_dir = config['data_dir']

    # ä»…æ”¯æŒ HuggingFace datasets ç®¡çº¿ï¼ˆä¸ç¤¾åŒºå®è·µä¸€è‡´ï¼‰
    if config.get('use_hf_data', False):
        if not HAS_DATASETS:
            raise ImportError("éœ€è¦å®‰è£… datasets: pip install datasets")

        from datasets import Dataset
        
        # ç›´æ¥ä»æœ¬åœ°æ–‡æœ¬åŠ è½½ï¼ˆä¸å°è¯•è¿æ¥HuggingFace Hubï¼‰
        def _read_parallel(split: str):
            de_path = os.path.join(data_dir, f"{split}.de")
            en_path = os.path.join(data_dir, f"{split}.en")
            if not (os.path.exists(de_path) and os.path.exists(en_path)):
                raise FileNotFoundError(
                    f"æœªæ‰¾åˆ°æœ¬åœ° {split}.de/.enï¼Œè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨äº {data_dir}"
                )
            de_list: List[str] = []
            en_list: List[str] = []
            with open(de_path, 'r', encoding='utf-8') as f_de, open(en_path, 'r', encoding='utf-8') as f_en:
                for de_line, en_line in zip(f_de, f_en):
                    de_txt = de_line.strip()
                    en_txt = en_line.strip()
                    if de_txt and en_txt:
                        de_list.append(de_txt)
                        en_list.append(en_txt)
            if len(de_list) == 0:
                raise ValueError(f"æœ¬åœ° {split} æ–‡æœ¬ä¸ºç©º")
            print(f"ğŸ“‚ ä»æœ¬åœ°åŠ è½½ {split}: {len(de_list)} å¥å¯¹")
            return Dataset.from_dict({'de': de_list, 'en': en_list})
        
        train_raw = _read_parallel('train')
        valid_raw = _read_parallel('valid')

        # åŠ è½½åˆ†è¯å™¨ï¼ˆåˆ†è¯å™¨è®­ç»ƒå·²è§£è€¦åˆ° preprocess.pyï¼‰
        special_tokens = ['<pad>', '<unk>', '<s>', '</s>']
        tok_backend = config.get('tokenizer_backend', 'sentencepiece')  # sentencepiece | bpe

        def _get_text(ex, key):
            if 'translation' in ex:
                return ex['translation'][key]
            return ex[key]

        # æ ¹æ®åç«¯åŠ è½½åˆ†è¯å™¨
        if tok_backend == 'sentencepiece':
            if not HAS_SPM:
                raise ImportError("éœ€è¦å®‰è£… sentencepiece: pip install sentencepiece")
            spm_model = os.path.join(data_dir, 'spm_shared.model')
            if not os.path.exists(spm_model):
                raise FileNotFoundError(
                    f"âŒ åˆ†è¯å™¨ä¸å­˜åœ¨: {spm_model}\n"
                    f"   è¯·å…ˆè¿è¡Œé¢„å¤„ç†: python preprocess.py"
                )
            print(f"ğŸ“‚ åŠ è½½ SentencePiece åˆ†è¯å™¨: {spm_model}")
            sp = spm.SentencePieceProcessor(model_file=spm_model)
            src_tokenizer = SPWrapper(sp)
            tgt_tokenizer = src_tokenizer
        else:  # bpe (HuggingFace)
            shared_path = os.path.join(data_dir, 'tokenizer_shared.json')
            if not os.path.exists(shared_path):
                raise FileNotFoundError(
                    f"âŒ åˆ†è¯å™¨ä¸å­˜åœ¨: {shared_path}\n"
                    f"   è¯·å…ˆè¿è¡Œé¢„å¤„ç†: python preprocess.py --backend bpe"
                )
            print(f"ğŸ“‚ åŠ è½½ HuggingFace BPE åˆ†è¯å™¨: {shared_path}")
            src_tokenizer = Tokenizer.from_file(shared_path)
            # ç¡®ä¿è§£ç å™¨å­˜åœ¨
            try:
                if getattr(src_tokenizer, 'decoder', None) is None:
                    src_tokenizer.decoder = decoders.ByteLevel()
            except Exception:
                pass
            tgt_tokenizer = src_tokenizer
        
        # æ ¡éªŒç‰¹æ®Štoken
        missing = [t for t in special_tokens if src_tokenizer.token_to_id(t) is None]
        if missing:
            raise ValueError(f"åˆ†è¯å™¨ç¼ºå°‘ç‰¹æ®Štoken: {missing}")

        # ID å¸¸é‡
        src_pad = src_tokenizer.token_to_id('<pad>')
        tgt_pad = tgt_tokenizer.token_to_id('<pad>')
        bos_id = tgt_tokenizer.token_to_id('<s>')
        eos_id = tgt_tokenizer.token_to_id('</s>')
        unk_id = tgt_tokenizer.token_to_id('<unk>')

        max_src_len = int(config['max_src_len'])
        max_tgt_len = int(config['max_tgt_len'])

        num_proc = int(config.get('num_workers', 8))
        num_proc = max(1, num_proc // 2)  # map ä¸å®œå¼€å¤ªå¤šè¿›ç¨‹

        def _encode_example(ex):
            de_txt = _get_text(ex, 'de')
            en_txt = _get_text(ex, 'en')
            if not de_txt or not en_txt:
                return {'src_ids': [], 'tgt_ids': [], 'length': 0}
            
            # æºåºåˆ—ç¼–ç ï¼šåŠ ä¸Š EOSï¼ˆè®ºæ–‡æ ‡å‡†åšæ³•ï¼‰
            # Source: [word1, word2, ..., EOS]
            src_core = src_tokenizer.encode(de_txt).ids
            if config.get('drop_too_long', True) and len(src_core) + 1 > max_src_len:  # +1 for EOS
                return {'src_ids': [], 'tgt_ids': []}
            src_core = src_core[:max_src_len - 1]  # ç•™ä¸€ä¸ªä½ç½®ç»™ EOS
            src_ids = src_core + [eos_id]  # âœ… æºåºåˆ—åŠ ä¸Š EOS
            
            # ç›®æ ‡åºåˆ—ç¼–ç ï¼šBOS + content + EOSï¼ˆè®ºæ–‡æ ‡å‡†åšæ³•ï¼‰
            # Target: [BOS, word1, word2, ..., EOS]
            inner_max = max(0, max_tgt_len - 2)  # ç•™ä½ç½®ç»™ BOS å’Œ EOS
            tgt_core = tgt_tokenizer.encode(en_txt).ids
            
            # å¥å¯¹é•¿åº¦è¿‡æ»¤ï¼ˆå¸¸è§å®è·µï¼‰ï¼šè¿‡åº¦ä¸åŒ¹é…çš„å¥å¯¹å¯ä¸¢å¼ƒ
            if config.get('drop_too_long', True):
                src_len = len(src_core) if src_core else 1
                tgt_len = len(tgt_core) if tgt_core else 1
                ratio = max(src_len / tgt_len, tgt_len / src_len)
                if ratio > float(config.get('length_ratio_threshold', 2.0)):
                    return {'src_ids': [], 'tgt_ids': []}
            
            tgt_core = tgt_core[:inner_max]
            tgt_ids = [bos_id] + tgt_core + [eos_id]
            return {'src_ids': src_ids, 'tgt_ids': tgt_ids}

        print("ğŸ”„ å¯¹è®­ç»ƒ/éªŒè¯é›†è¿›è¡Œåˆ†è¯æ˜ å°„ï¼ˆdatasets.mapï¼‰â€¦")
        # æ¢å¤é»˜è®¤ç¼“å­˜æœºåˆ¶ï¼ˆload_from_cache_file=None/Trueï¼‰
        # åªè¦ä¹‹å‰çš„å¤„ç†å‚æ•°æ²¡å˜ï¼Œdatasetsä¼šè‡ªåŠ¨åŠ è½½å·²æœ‰çš„ç¼“å­˜ï¼Œä¸ä¼šé‡æ–°è·‘7ä¸ªå°æ—¶
        train_enc = train_raw.map(_encode_example, remove_columns=train_raw.column_names, num_proc=num_proc)
        valid_enc = valid_raw.map(_encode_example, remove_columns=valid_raw.column_names, num_proc=num_proc)

        # è¿‡æ»¤ç©ºæˆ–è¿‡é•¿è¢«ä¸¢å¼ƒçš„æ ·æœ¬
        train_enc = train_enc.filter(lambda ex: len(ex['src_ids']) > 0 and len(ex['tgt_ids']) > 1)
        valid_enc = valid_enc.filter(lambda ex: len(ex['src_ids']) > 0 and len(ex['tgt_ids']) > 1)

        # è½¬ä¸ºtorchæ ¼å¼ (ç§»é™¤ length åˆ—è¦æ±‚ï¼ŒåŒ¹é…æ—§ç¼“å­˜)
        train_enc.set_format(type='torch', columns=['src_ids', 'tgt_ids'])
        valid_enc.set_format(type='torch', columns=['src_ids', 'tgt_ids'])

        # Collatorï¼šä¿ç•™ç°æœ‰å®ç°ï¼ŒåŠ¨æ€æŒ‰batchè£å‰ª
        collator = Collator(src_pad_token_id=src_pad, tgt_pad_token_id=tgt_pad)

        # æ„å»ºæ‰¹é‡‡æ ·å™¨ï¼ˆé•¿åº¦åŸºäºå®é™…åºåˆ—é•¿åº¦ï¼›æ­¤å¤„ä¸ä¾èµ–padï¼‰
        max_tokens = config['max_tokens_per_batch']
        max_sentences = config.get('max_sentences_per_batch')

        train_sampler = TokenBatchSampler(
            train_enc, max_tokens_per_batch=max_tokens, max_sentences_per_batch=max_sentences,
            shuffle=True
        )
        val_sampler = TokenBatchSampler(
            valid_enc, max_tokens_per_batch=max_tokens, max_sentences_per_batch=max_sentences,
            shuffle=False
        )

        num_workers = config.get('num_workers', 8)
        train_loader = DataLoader(
            train_enc, batch_sampler=train_sampler, collate_fn=collator,
            num_workers=num_workers, pin_memory=torch.cuda.is_available(),
            persistent_workers=(num_workers > 0)
        )
        val_loader = DataLoader(
            valid_enc, batch_sampler=val_sampler, collate_fn=collator,
            num_workers=num_workers, pin_memory=torch.cuda.is_available(),
            persistent_workers=(num_workers > 0)
        )

        vocab_info = {
            'src_vocab_size': src_tokenizer.get_vocab_size(),
            'tgt_vocab_size': tgt_tokenizer.get_vocab_size(),
            'pad_token_id': tgt_pad,
            'src_pad_token_id': src_pad,
            'tgt_pad_token_id': tgt_pad,
            'bos_token_id': bos_id,
            'eos_token_id': eos_id,
            'unk_token_id': unk_id,
            # å…±äº«è¯è¡¨æ—¶ src çš„ BOS/EOS ä¸ tgt ç›¸åŒï¼Œç”¨äºè¯„ä¼°å›é€€
            'src_bos_token_id': src_tokenizer.token_to_id('<s>'),
            'src_eos_token_id': src_tokenizer.token_to_id('</s>'),
        }

        print("âœ… DataLoaderåˆ›å»ºå®Œæˆ (datasets ç®¡çº¿)")
        print(f"   è®­ç»ƒ: {len(train_enc):,} æ ·æœ¬")
        print(f"   éªŒè¯: {len(valid_enc):,} æ ·æœ¬")
        print(f"   è¯æ±‡: src={vocab_info['src_vocab_size']}, tgt={vocab_info['tgt_vocab_size']}")

        return train_loader, val_loader, vocab_info

    # ä¸å†æ”¯æŒæ—§ç¼“å­˜è·¯å¾„/ç¦»çº¿é¢„å¤„ç†
    raise FileNotFoundError("ä»…æ”¯æŒ datasets ç®¡çº¿ï¼šè¯·åœ¨ config ä¸­è®¾ç½® use_hf_data=True")
