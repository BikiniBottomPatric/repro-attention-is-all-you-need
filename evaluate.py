#!/usr/bin/env python3
"""
ç‹¬ç«‹è¯„ä¼°è„šæœ¬ - ä¸“é—¨ç”¨äºè®¡ç®—BLEUåˆ†æ•°å’Œå…¶ä»–è¯„ä¼°æŒ‡æ ‡
ä¸è®­ç»ƒè¿‡ç¨‹å®Œå…¨è§£è€¦ï¼Œå¯ä»¥å•ç‹¬è¿è¡Œ

é…ç½®ç®¡ç†:
    - é»˜è®¤å‚æ•°ä» config.py åŠ è½½
    - å‘½ä»¤è¡Œå‚æ•°å¯ä»¥è¦†ç›–é»˜è®¤å€¼

ä½¿ç”¨æ–¹æ³•:
    python evaluate.py --model checkpoints/best_model.pt
    python evaluate.py --model checkpoints/best_model.pt --method beam --batch_size 8
"""

import os
import torch
import sacrebleu
import argparse
# å¯é€‰çš„è¿›åº¦æ¡æ”¯æŒ
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
from inference import TransformerInference
from config import get_config
from utils import load_model_and_tokenizers  # ä½¿ç”¨å…±äº«å‡½æ•°é¿å…é‡å¤

 

def evaluate_bleu(model, src_tokenizer, tgt_tokenizer, vocab_info, device, 
                 data_dir, batch_size=32, method='greedy', beam_size=4, max_length=256, length_penalty=0.6, split='test'):
    """åœ¨æŒ‡å®šæ‹†åˆ†ä¸Šè¯„ä¼°BLEUåˆ†æ•° (split: 'valid' | 'test')"""
    split_name = 'éªŒè¯é›†' if split == 'valid' else 'æµ‹è¯•é›†'
    print(f"BLEUè¯„ä¼°: {split_name}, method={method}, bs={batch_size}")
    if method == 'beam':
        print(f"beam={beam_size}, alpha={length_penalty}")
    
    # åˆ›å»ºæ¨ç†å™¨
    inference = TransformerInference(
        model, src_tokenizer, tgt_tokenizer, device,
        vocab_info['bos_token_id'], vocab_info['eos_token_id'], vocab_info['pad_token_id']
    )
    
    # åŠ è½½åŸå§‹æ–‡æœ¬ä»¥ä½œå‚è€ƒï¼ˆä¼˜å…ˆï¼‰
    src_texts = []
    ref_texts = []
    src_txt = os.path.join(data_dir, f'{split}.de')
    tgt_txt = os.path.join(data_dir, f'{split}.en')
    use_raw_refs = os.path.exists(src_txt) and os.path.exists(tgt_txt)

    if use_raw_refs:
        print("è¯»å–åŸå§‹å‚è€ƒ...")
        with open(src_txt, 'r', encoding='utf-8') as f_src, open(tgt_txt, 'r', encoding='utf-8') as f_tgt:
            for s, r in zip(f_src, f_tgt):
                s_stripped = s.strip()
                r_stripped = r.strip()
                if s_stripped and r_stripped:
                    src_texts.append(s_stripped)
                    ref_texts.append(r_stripped)
        total_samples = len(src_texts)
        print(f"ğŸ“Š æ ·æœ¬æ•°: {total_samples}")
    else:
        # ç›´æ¥ä»æœ¬åœ°æ–‡æœ¬è¯»å–ï¼ˆä¸å°è¯•è¿æ¥HuggingFaceï¼‰
        de_path = os.path.join(data_dir, f"{split}.de")
        en_path = os.path.join(data_dir, f"{split}.en")
        if not (os.path.exists(de_path) and os.path.exists(en_path)):
            print(f"âŒ æ— æ³•è·å–å‚è€ƒï¼šè¯·ç¡®ä¿å­˜åœ¨ {split}.de å’Œ {split}.en")
            return 0.0
        print("ä»æœ¬åœ°å¹³è¡Œæ–‡æœ¬åŠ è½½å‚è€ƒ...")
        with open(de_path, 'r', encoding='utf-8') as f_de, open(en_path, 'r', encoding='utf-8') as f_en:
            for de_line, en_line in zip(f_de, f_en):
                de_txt = de_line.strip()
                en_txt = en_line.strip()
                if de_txt and en_txt:
                    src_texts.append(de_txt)
                    ref_texts.append(en_txt)
        print(f"ğŸ“Š æ ·æœ¬æ•°: {len(src_texts)}")
    
    # æ‰¹é‡ç”Ÿæˆç¿»è¯‘
    print(f"ç”Ÿæˆç¿»è¯‘ ({method}) ...")
    predictions = []
    model.eval()
    
    with torch.no_grad():
        iterator = range(0, len(src_texts), batch_size)
        if HAS_TQDM:
            iterator = tqdm(iterator, total=(len(src_texts) + batch_size - 1) // batch_size, desc="æ¨ç†ä¸­")
        for i in iterator:
            batch_texts = src_texts[i:i+batch_size]
            
            # æ ¹æ®æ–¹æ³•é€‰æ‹©åˆé€‚çš„å‚æ•°
            if method == 'beam':
                batch_preds = inference.translate_batch(
                    batch_texts, method, max_length, beam_size, length_penalty
                )
            else:
                batch_preds = inference.translate_batch(
                    batch_texts, method, max_length
                )
            
            predictions.extend(batch_preds)
            
            if not HAS_TQDM:
                # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ— tqdmæ—¶ï¼‰
                progress = min(i + batch_size, len(src_texts))
                print(f"è¿›åº¦: {progress}/{len(src_texts)} ({100*progress/len(src_texts):.1f}%)")
    
    # è®¡ç®—BLEUåˆ†æ•°ï¼ˆç›´æ¥ä½¿ç”¨ tokenizer.decode çš„è¾“å‡ºï¼‰
    print("è®¡ç®—BLEU...")
    if predictions and ref_texts:
        bleu = sacrebleu.corpus_bleu(predictions, [ref_texts], tokenize='13a')
        print(f"âœ… BLEUåˆ†æ•°: {bleu.score:.2f}")
        
        # æ˜¾ç¤ºä¸€äº›ç¿»è¯‘ç¤ºä¾‹
        print("\nğŸ“ ç¿»è¯‘ç¤ºä¾‹:")
        for i in range(min(3, len(predictions))):
            print(f"æºæ–‡: {src_texts[i]}")
            print(f"å‚è€ƒ: {ref_texts[i]}")
            print(f"ç¿»è¯‘: {predictions[i]}")
            print("-" * 50)
        
        return bleu.score
    else:
        print("âŒ ç¿»è¯‘ç»“æœä¸ºç©º")
        return 0.0

def main():
    # åŠ è½½é…ç½®ä½œä¸ºé»˜è®¤å€¼
    config = get_config()
    
    parser = argparse.ArgumentParser(description='Transformeræ¨¡å‹è¯„ä¼°')
    parser.add_argument('--model', required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data_dir', default=config['data_dir'], help='æ•°æ®ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=config['eval_batch_size'], 
                       help=f'æ¨ç†æ‰¹å¤„ç†å¤§å° (é»˜è®¤: {config["eval_batch_size"]}, æ ¹æ®GPUè°ƒæ•´)')
    parser.add_argument('--method', choices=['greedy', 'beam'], default=config['eval_method'], 
                       help=f'è§£ç æ–¹æ³• (é»˜è®¤: {config["eval_method"]}): greedy (å¿«é€Ÿ) æˆ– beam (é«˜è´¨é‡)')
    parser.add_argument('--beam_size', type=int, default=config['eval_beam_size'], 
                       help=f'æŸæœç´¢å¤§å° (é»˜è®¤: {config["eval_beam_size"]}, ä»…beamæ¨¡å¼)')
    parser.add_argument('--max_length', type=int, default=config['eval_max_length'], 
                       help=f'æœ€å¤§ç”Ÿæˆé•¿åº¦ (é»˜è®¤: {config["eval_max_length"]})')
    parser.add_argument('--length_penalty', type=float, default=config['eval_length_penalty'], 
                       help=f'é•¿åº¦æƒ©ç½šç³»æ•° (é»˜è®¤: {config["eval_length_penalty"]}, tensor2tensorå®˜æ–¹è®¾ç½®)')
    parser.add_argument('--split', choices=['valid', 'test'], default='test',
                       help='è¯„ä¼°æ•°æ®æ‹†åˆ†: valid(å¿«) æˆ– test(æœ€ç»ˆæŠ¥å‘Š)')
    
    args = parser.parse_args()
    
    print("ğŸ¯ ç‹¬ç«‹è¯„ä¼°è„šæœ¬ (æµ‹è¯•é›†: newstest2014)")
    print("=" * 50)
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(args.model):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        return
    
    try:
        # åŠ è½½æ¨¡å‹
        model, src_tokenizer, tgt_tokenizer, vocab_info, device = load_model_and_tokenizers(
            args.model, args.data_dir
        )
        
        # è¯„ä¼°BLEUï¼ˆå®Œæ•´æµ‹è¯•é›†ï¼‰
        bleu_score = evaluate_bleu(
            model, src_tokenizer, tgt_tokenizer, vocab_info, device,
            args.data_dir, args.batch_size, args.method, args.beam_size, args.max_length, args.length_penalty, args.split
        )
        
        print(f"\nğŸ† æœ€ç»ˆBLEUåˆ†æ•°: {bleu_score:.2f}")
        print(f"ğŸ“Š è¯„ä¼°é…ç½®:")
        print(f"   è§£ç æ–¹æ³•: {args.method}")
        print(f"   æµ‹è¯•é›†: å®Œæ•´æµ‹è¯•é›† (newstest2014)")
        print(f"   æ‰¹å¤§å°: {args.batch_size}")
        print(f"   æœ€å¤§é•¿åº¦: {args.max_length}")
        if args.method == 'beam':
            print(f"   æŸæœç´¢å¤§å°: {args.beam_size}")
            print(f"   é•¿åº¦æƒ©ç½š: {args.length_penalty} (tensor2tensorå®˜æ–¹è®¾ç½®)")
            print(f"   â­ ä½¿ç”¨æŸæœç´¢ (æ›´é«˜è´¨é‡ä½†é€Ÿåº¦è¾ƒæ…¢)")
        else:
            print(f"   âš¡ ä½¿ç”¨è´ªå¿ƒè§£ç  (é€Ÿåº¦å¿«)")
        print(f"\nğŸ’¡ æç¤º: å¯é€šè¿‡ä¿®æ”¹ config.py æ›´æ”¹é»˜è®¤è¯„ä¼°å‚æ•°")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
