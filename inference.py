"""
æ¨ç†è„šæœ¬ - ç®€æ´çš„æ¨¡å‹æ¨ç†å’Œç¿»è¯‘
æ”¯æŒè´ªå¿ƒè§£ç ã€æŸæœç´¢è§£ç ã€äº¤äº’å¼ç¿»è¯‘
"""

import torch
import torch.nn.functional as F
from tokenizers import Tokenizer
from typing import List
import os

from utils import load_model_and_tokenizers
try:
    from torch.amp import autocast as amp_autocast  # PyTorch 2.x
except Exception:
    try:
        from torch.cuda.amp import autocast as amp_autocast  # 1.x fallback
    except Exception:
        amp_autocast = None


class TransformerInference:
    """Transformeræ¨ç†å™¨"""
    
    def __init__(
        self,
        model,
        src_tokenizer: Tokenizer,
        tgt_tokenizer: Tokenizer,
        device: torch.device,
        bos_id: int = 2,
        eos_id: int = 3,
        pad_id: int = 0
    ):
        self.model = model.to(device)  # ç¡®ä¿modelåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        self.src_tokenizer = src_tokenizer
        self.tgt_tokenizer = tgt_tokenizer
        self.device = device
        self.bos_id = bos_id
        self.eos_id = eos_id
        self.pad_id = pad_id
        
        self.model.eval()
    
    def encode_text(self, text: str) -> torch.Tensor:
        """ç¼–ç è¾“å…¥æ–‡æœ¬ - åŠ ä¸ŠEOSï¼ˆä¸è®­ç»ƒæ ¼å¼ä¸€è‡´ï¼‰"""
        tokens = self.src_tokenizer.encode(text).ids
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæºåºåˆ—å¿…é¡»åŠ ä¸Š EOSï¼Œä¸è®­ç»ƒæ—¶çš„æ•°æ®æ ¼å¼ä¸€è‡´
        # æ³¨æ„ï¼šSentencePiece å¦‚æœè®­ç»ƒæ—¶æœªå¼€å¯ add_eosï¼Œåˆ™éœ€è¦æ‰‹åŠ¨åŠ ã€‚
        # è¿™é‡Œå‡è®¾åˆ†è¯å™¨æ²¡æœ‰è‡ªåŠ¨åŠ  EOSã€‚
        if tokens and tokens[-1] != self.eos_id:
             tokens = tokens + [self.eos_id]
        return torch.tensor([tokens], dtype=torch.long, device=self.device)
    
    def decode_tokens(self, token_ids: List[int]) -> str:
        """è§£ç tokenåºåˆ—ä¸ºæ–‡æœ¬"""
        # è¿‡æ»¤ç‰¹æ®Štoken
        filtered = []
        special_tokens = [self.bos_id, self.eos_id, self.pad_id]
        for t in token_ids:
            if t not in special_tokens:
                filtered.append(t)
        return self.tgt_tokenizer.decode(filtered).strip()
    
    def greedy_decode(self, text: str, max_length: int = None) -> str:
        """è´ªå¿ƒè§£ç  - å•æ¡æ–‡æœ¬ï¼ˆè°ƒç”¨æ‰¹é‡ç‰ˆæœ¬ï¼‰"""
        results = self.greedy_decode_batch([text], max_length)
        return results[0] if results else ""

    def greedy_decode_batch(self, texts: List[str], max_length: int = None) -> List[str]:
        """æ‰¹é‡è´ªå¿ƒè§£ç  - å¸¦æ¸è¿›å¼EOSåç½®"""
        if not texts:
            return []
        from config import get_config
        config = get_config()
        if max_length is None:
            max_length = config.get('eval_max_length', 100)
        # è¯»å–æŠ‘åˆ¶ä¸æƒ©ç½šé…ç½®
        # min_len = int(config.get('min_decode_length', 3))
        # no_repeat_ngram = int(config.get('no_repeat_ngram_size', 3))
        # eos_bias = float(config.get('eos_bias', 0.0))
        # repetition_penalty = float(config.get('repetition_penalty', 1.2))
        
        # è·å–UNK token IDç”¨äºæŠ‘åˆ¶
        unk_id = self.tgt_tokenizer.token_to_id('<unk>') if hasattr(self.tgt_tokenizer, 'token_to_id') else 1

        with torch.no_grad():
            # æ‰¹é‡ç¼–ç æºæ–‡æœ¬å¹¶padding
            encs = self.src_tokenizer.encode_batch(texts)
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæºåºåˆ—å¿…é¡»åŠ ä¸Š EOSï¼Œä¸è®­ç»ƒæ—¶çš„æ•°æ®æ ¼å¼ä¸€è‡´
            src_ids = []
            for e in encs:
                ids = e.ids
                if ids and ids[-1] != self.eos_id:
                    ids = ids + [self.eos_id]
                src_ids.append(ids)
            
            max_src_len = max((len(x) for x in src_ids), default=1)
            batch_size = len(src_ids)
            
            
            src_seq = torch.full((batch_size, max_src_len), self.pad_id, dtype=torch.long, device=self.device)
            for i, ids in enumerate(src_ids):
                if ids:
                    src_seq[i, :len(ids)] = torch.tensor(ids, dtype=torch.long, device=self.device)

            # é¢„è®¡ç®—encoderè¾“å‡º
            # if amp_autocast and self.device.type == 'cuda':
            #     with amp_autocast('cuda'):
            #         encoder_output, src_mask = self.model.encode(src_seq)
            # else:
            encoder_output, src_mask = self.model.encode(src_seq)

            # é¢„åˆ†é…ç›®æ ‡åºåˆ—
            tgt_seq = torch.full((batch_size, max_length), self.pad_id, dtype=torch.long, device=self.device)
            tgt_seq[:, 0] = self.bos_id
            finished = torch.zeros(batch_size, dtype=torch.bool, device=self.device)
            cur_len = 1

            for _ in range(1, max_length):
                active_seq = tgt_seq[:, :cur_len]
                # if amp_autocast and self.device.type == 'cuda':
                #     with amp_autocast('cuda'):
                #         dec_out = self.model.decode(active_seq, encoder_output, src_mask)
                #         logits = self.model.output_projection(dec_out)
                # else:
                dec_out = self.model.decode(active_seq, encoder_output, src_mask)
                logits = self.model.output_projection(dec_out)

                last_logits = logits[:, -1, :].clone()
                
                # 1. é‡å¤æƒ©ç½šï¼ˆé€æ ·æœ¬ï¼‰
                # if repetition_penalty != 1.0 and cur_len > 1:
                #     for i in range(batch_size):
                #         if finished[i]:
                #             continue
                #         prev_tokens = tgt_seq[i, 1:cur_len].tolist()
                #         for token_id in set(prev_tokens):
                #             if last_logits[i, token_id] > 0:
                #                 last_logits[i, token_id] = last_logits[i, token_id] / repetition_penalty
                #             else:
                #                 last_logits[i, token_id] = last_logits[i, token_id] * repetition_penalty
                
                # EOS åç½®ï¼ˆä»…å½“é…ç½®ä¸­ eos_bias > 0 æ—¶ç”Ÿæ•ˆï¼‰
                # if eos_bias > 0:
                #     last_logits[:, self.eos_id] = last_logits[:, self.eos_id] + eos_bias
                
                # æœ€å°é•¿åº¦é™åˆ¶
                # if cur_len < min_len:
                #     last_logits[:, self.eos_id] = -1e4
                
                # 3. å·²å®Œæˆæ ·æœ¬ä»…å…è®¸EOS
                if finished.any():
                    last_logits[finished] = -1e4  # å…¼å®¹ fp16
                    last_logits[finished, self.eos_id] = 0.0
                
                # UNK æŠ‘åˆ¶å·²ç§»é™¤ï¼ˆå’Œ example ä¸€è‡´ï¼‰
                # if unk_id is not None:
                #     last_logits[:, unk_id] = -1e4
                
                # 5. n-gramé‡å¤æŠ‘åˆ¶ï¼ˆé€æ ·æœ¬ï¼‰
                # if no_repeat_ngram > 1 and cur_len >= no_repeat_ngram - 1:
                #     window = no_repeat_ngram - 1
                #     for i in range(batch_size):
                #         if finished[i]:
                #             continue
                #         hist = tgt_seq[i, :cur_len].tolist()
                #         ng_map = {}
                #         for j in range(len(hist) - window):
                #             prefix = tuple(hist[j:j+window])
                #             nxt = hist[j+window]
                #             s = ng_map.get(prefix)
                #             if s is None:
                #                 ng_map[prefix] = {nxt}
                #             else:
                #                 s.add(nxt)
                #         cur_prefix = tuple(hist[-window:])
                #         banned = ng_map.get(cur_prefix)
                #         if banned:
                #             last_logits[i, list(banned)] = -1e4

                next_tokens = torch.argmax(last_logits, dim=-1)
                next_tokens = torch.where(finished, torch.full_like(next_tokens, self.eos_id), next_tokens)
                tgt_seq[:, cur_len] = next_tokens
                finished |= (next_tokens == self.eos_id)
                cur_len += 1
                if bool(finished.all()):
                    break

            # è§£ç æ–‡æœ¬
            results: List[str] = []
            for i in range(batch_size):
                results.append(self.decode_tokens(tgt_seq[i, :cur_len].tolist()))
            return results
    
    def beam_search_decode(
        self,
        text: str,
        beam_size: int = 4,
        max_length: int = None,
        alpha: float = None
    ) -> str:
        """æŸæœç´¢è§£ç  - å•æ¡æ–‡æœ¬ï¼ˆè°ƒç”¨æ‰¹é‡ç‰ˆæœ¬ï¼‰"""
        results = self.beam_search_decode_batch([text], beam_size, max_length, alpha)
        return results[0] if results else ""
    
    def beam_search_decode_batch(
        self,
        texts: List[str],
        beam_size: int = 4,
        max_length: int = None,
        alpha: float = None
    ) -> List[str]:
        """è·¨æ ·æœ¬Ã—å¤šæŸçš„æ‰¹é‡æŸæœç´¢è§£ç ï¼ˆå‘é‡åŒ–å®ç°ï¼‰"""
        if not texts:
            return []
        if max_length is None:
            from config import get_config
            config = get_config()
            max_length = config.get('eval_max_length', 100)
        if alpha is None:
            from config import get_config as _gc
            _cfg = _gc()
            alpha = _cfg.get('eval_length_penalty', 0.6)

        device = self.device
        eos_id = self.eos_id
        pad_id = self.pad_id
        bos_id = self.bos_id

        with torch.no_grad():
            # 1) æ‰¹é‡ç¼–ç æºæ–‡æœ¬
            encs = self.src_tokenizer.encode_batch(texts)
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæºåºåˆ—å¿…é¡»åŠ ä¸Š EOSï¼Œä¸è®­ç»ƒæ—¶çš„æ•°æ®æ ¼å¼ä¸€è‡´
            src_ids = []
            for e in encs:
                ids = e.ids
                if ids and ids[-1] != eos_id:
                    ids = ids + [eos_id]
                src_ids.append(ids)
            
            B = len(src_ids)
            S_max = max((len(x) for x in src_ids), default=1)
            
            # ç›´æ¥ä½¿ç”¨ max_lengthï¼Œä¸åšåŠ¨æ€é™åˆ¶ï¼ˆå’Œ example ä¸€è‡´ï¼‰
            effective_max_length = max_length
            
            src_seq = torch.full((B, S_max), pad_id, dtype=torch.long, device=device)
            for i, ids in enumerate(src_ids):
                if ids:
                    src_seq[i, :len(ids)] = torch.tensor(ids, dtype=torch.long, device=device)

            # 2) é¢„è®¡ç®—encoderè¾“å‡º
            # if amp_autocast and device.type == 'cuda':
            #     with amp_autocast('cuda'):
            #         base_enc_out, base_src_mask = self.model.encode(src_seq)
            # else:
            base_enc_out, base_src_mask = self.model.encode(src_seq)

            # 3) åˆå§‹åŒ–beamå®¹å™¨
            beam = int(beam_size)
            # sequences: (B, beam, 1)
            sequences = torch.full((B, beam, 1), bos_id, dtype=torch.long, device=device)
            # scores: (B, beam) - åˆå§‹ä»…ç¬¬0æŸæœ‰æ•ˆï¼Œå…¶ä½™ç½®ä¸ºæå°
            scores = torch.full((B, beam), -1e4, dtype=torch.float32, device=device)
            scores[:, 0] = 0.0
            finished = torch.zeros((B, beam), dtype=torch.bool, device=device)

            # é¢„å±•å¼€encoderè¾“å‡ºåˆ°beamç»´ï¼ˆæŒ‰éœ€åœ¨æ¯æ­¥flattenä½¿ç”¨ï¼‰
            # base_enc_out: (B, S, D) -> (B, beam, S, D)
            # base_src_mask: (B, 1, 1, S) -> (B, beam, 1, 1, S)
            enc_out_beam = base_enc_out.unsqueeze(1).expand(B, beam, base_enc_out.size(1), base_enc_out.size(2))
            src_mask_beam = base_src_mask.unsqueeze(1).expand(B, beam, base_src_mask.size(1), base_src_mask.size(2), base_src_mask.size(3))

            # å¯é€‰ï¼šn-gramé‡å¤æŠ‘åˆ¶ä¸æœ€å°é•¿åº¦
            from config import get_config as _gc2
            _cf2 = _gc2()
            # no_repeat_ngram = int(_cf2.get('no_repeat_ngram_size', 3))
            # min_len = int(_cf2.get('min_decode_length', 1))
            # eos_bias = float(_cf2.get('eos_bias', 0.0))  # EOSæ¦‚ç‡æå‡
            # repetition_penalty = float(_cf2.get('repetition_penalty', 1.2))  # é‡å¤æƒ©ç½š
            
            # è·å–UNK token IDç”¨äºæŠ‘åˆ¶
            unk_id = self.tgt_tokenizer.token_to_id('<unk>') if hasattr(self.tgt_tokenizer, 'token_to_id') else 1

            cur_len = 1
            for _ in range(1, effective_max_length):
                # 4) å‡†å¤‡decoderè¾“å…¥ (B*beam, cur_len)
                dec_in = sequences.view(B * beam, cur_len)
                enc_out_flat = enc_out_beam.contiguous().view(B * beam, enc_out_beam.size(2), enc_out_beam.size(3))
                src_mask_flat = src_mask_beam.contiguous().view(B * beam, src_mask_beam.size(2), src_mask_beam.size(3), src_mask_beam.size(4))

                # 5) å‰å‘è®¡ç®—æœ€åä½ç½®logits
                # if amp_autocast and device.type == 'cuda':
                #     with amp_autocast('cuda'):
                #         dec_out = self.model.decode(dec_in, enc_out_flat, src_mask_flat)
                #         logits = self.model.output_projection(dec_out)
                # else:
                dec_out = self.model.decode(dec_in, enc_out_flat, src_mask_flat)
                logits = self.model.output_projection(dec_out)

                last_logits = logits[:, -1, :]  # (B*beam, V)
                log_probs = F.log_softmax(last_logits, dim=-1)
                
                # EOS åç½®ï¼ˆä»…å½“é…ç½®ä¸­ eos_bias > 0 æ—¶ç”Ÿæ•ˆï¼‰
                # if eos_bias > 0:
                #     log_probs[:, eos_id] = log_probs[:, eos_id] + eos_bias
                
                # 6) å·²å®ŒæˆæŸä»…å…è®¸äº§ç”ŸEOS
                finished_flat = finished.view(B * beam)
                if finished_flat.any():
                    log_probs[finished_flat] = -1e4
                    log_probs[finished_flat, eos_id] = 0.0

                # 6.1) æœ€å°é•¿åº¦å‰ç¦æ­¢EOS
                # if cur_len < min_len:
                #     log_probs[:, eos_id] = -1e4

                # UNK æŠ‘åˆ¶å·²ç§»é™¤ï¼ˆå’Œ example ä¸€è‡´ï¼‰
                # if unk_id is not None:
                #     log_probs[:, unk_id] = -1e4

                # 6.2) n-gram é‡å¤æŠ‘åˆ¶ï¼ˆé€æŸå¤„ç†ï¼Œå¼€é”€å°ï¼‰
                # if no_repeat_ngram > 1 and cur_len + 1 >= no_repeat_ngram:
                #     V = log_probs.size(-1)
                #     # sequences: (B, beam, cur_len)
                #     seq_flat = sequences.view(B * beam, cur_len)
                #     window = no_repeat_ngram - 1
                #     for idx in range(B * beam):
                #         hist = seq_flat[idx].tolist()
                #         # å»ºç«‹å·²å‡ºç°çš„ n-gram æ˜ å°„: prefix -> {next}
                #         ng_map = {}
                #         for j in range(len(hist) - window):
                #             prefix = tuple(hist[j:j+window])
                #             nxt = hist[j+window]
                #             s = ng_map.get(prefix)
                #             if s is None:
                #                 ng_map[prefix] = {nxt}
                #             else:
                #                 s.add(nxt)
                #         cur_prefix = tuple(hist[-window:])
                #         banned = ng_map.get(cur_prefix)
                #         if banned:
                #             log_probs[idx, list(banned)] = -1e4
                
                # 6.3) é‡å¤æƒ©ç½šï¼šé™ä½å·²ç”Ÿæˆtokençš„æ¦‚ç‡
                # if repetition_penalty != 1.0 and cur_len > 1:
                #     seq_flat = sequences.view(B * beam, cur_len)
                #     for idx in range(B * beam):
                #         hist = seq_flat[idx].tolist()
                #         # è·³è¿‡BOSï¼Œå¯¹å·²å‡ºç°çš„tokenæ–½åŠ æƒ©ç½š
                #         seen_tokens = set(hist[1:])  # è·³è¿‡BOS
                #         for token_id in seen_tokens:
                #             if log_probs[idx, token_id] > 0:
                #                 log_probs[idx, token_id] = log_probs[idx, token_id] / repetition_penalty
                #             else:
                #                 log_probs[idx, token_id] = log_probs[idx, token_id] * repetition_penalty
                
                # 7) ç´¯ç§¯åˆ†æ•°å¹¶é€‰æ‹©topk
                V = log_probs.size(-1)
                log_probs = log_probs.view(B, beam, V)
                cand_scores = scores.unsqueeze(-1) + log_probs  # (B, beam, V)
                cand_scores = cand_scores.view(B, beam * V)

                topk_scores, topk_indices = torch.topk(cand_scores, k=beam, dim=-1)  # (B, beam)
                prev_beam_idx = topk_indices // V  # (B, beam)
                next_tokens = (topk_indices % V).to(torch.long)  # (B, beam)

                # 8) ç»„è£…æ–°åºåˆ—
                # ä»æ—§åºåˆ—ä¸­æŒ‰prev_beam_idxé€‰å– (gather)
                prev_seq = sequences  # (B, beam, cur_len)
                gather_idx = prev_beam_idx.unsqueeze(-1).expand(B, beam, cur_len)
                gathered = torch.gather(prev_seq, 1, gather_idx)
                sequences = torch.cat([gathered, next_tokens.unsqueeze(-1)], dim=-1)  # (B, beam, cur_len+1)

                # 9) æ›´æ–°scoresä¸finished
                scores = topk_scores
                newly_finished = next_tokens.eq(eos_id)
                finished = torch.gather(finished, 1, prev_beam_idx) | newly_finished

                cur_len += 1
                # å¦‚æœå…¨éƒ¨å®Œæˆï¼Œæå‰ç»“æŸ
                if bool(finished.all()):
                    break

            # 10) æŒ‰é•¿åº¦æƒ©ç½šé€‰æ‹©æ¯ä¸ªæ ·æœ¬çš„æœ€ä½³æŸ
            # è®¡ç®—æ¯æŸçš„æœ‰æ•ˆé•¿åº¦ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªEOSçš„ä½ç½®ï¼ˆæ²¡æœ‰åˆ™ä¸ºcur_lenï¼‰
            seqs_flat = sequences  # (B, beam, L)
            L = seqs_flat.size(-1)
            eos_mat = seqs_flat.eq(eos_id)
            # first eos position (index), default L-1 if none, length = idx+1
            # ä½¿ç”¨ä¸€ä¸ªå¤§ç´¢å¼•å¡«å……æœªå‡ºç°çš„ä½ç½®
            eos_pos = torch.where(eos_mat.any(dim=-1), eos_mat.float().argmax(dim=-1), torch.full((B, beam), L - 1, device=device, dtype=torch.long))
            lengths = (eos_pos + 1).to(torch.float32)

            lp = ((5.0 + lengths) ** alpha) / (6.0 ** alpha)
            norm_scores = scores / lp
            best_idx = norm_scores.argmax(dim=1)  # (B,)

            # é€‰å‡ºæœ€ä½³åºåˆ—
            gather_idx = best_idx.view(B, 1, 1).expand(B, 1, L)
            best_seqs = torch.gather(seqs_flat, 1, gather_idx).squeeze(1)  # (B, L)

            # è§£ç åˆ°æ–‡æœ¬
            results: List[str] = []
            for i in range(B):
                results.append(self.decode_tokens(best_seqs[i].tolist()))
            return results

    def translate_batch(self, texts: List[str], method: str = 'greedy', max_length: int = None, beam_size: int = 4, alpha: float = None) -> List[str]:
        """æ‰¹é‡ç¿»è¯‘"""
        # è®¾ç½®é»˜è®¤æœ€å¤§é•¿åº¦
        if max_length is None:
            from config import get_config
            config = get_config()
            max_length = config.get('eval_max_length', 100)  # åˆç†çš„é»˜è®¤å€¼ï¼Œé¿å…è¿‡é•¿ç”Ÿæˆ
        if alpha is None:
            from config import get_config as _gc
            _cfg = _gc()
            alpha = _cfg.get('eval_length_penalty', 0.6)
        
        if method == 'greedy':
            return self.greedy_decode_batch(texts, max_length)
        
        return self.beam_search_decode_batch(texts, beam_size, max_length, alpha)
    
    def interactive(self, default_max_length: int = None):
        """äº¤äº’å¼ç¿»è¯‘ - ç®€åŒ–ç‰ˆæœ¬"""
        print("ğŸ¯ äº¤äº’å¼ç¿»è¯‘ (è¾“å…¥ 'quit' é€€å‡º)")
        print("å‘½ä»¤:")
        print("  <æ–‡æœ¬> - è´ªå¿ƒè§£ç ç¿»è¯‘")
        print("  beam:<æ–‡æœ¬> - æŸæœç´¢ç¿»è¯‘")
        if default_max_length is None:
            from config import get_config
            config = get_config()
            default_max_length = config.get('eval_max_length', 100)
        print(f"  æœ€å¤§é•¿åº¦: {default_max_length}")
        print()
        
        while True:
            try:
                user_input = input("å¾·è¯­ >>> ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    break
                
                if not user_input:
                    continue
                
                # ç®€åŒ–çš„å‘½ä»¤è§£æ
                if user_input.startswith('beam:'):
                    text = user_input[5:].strip()
                    result = self.beam_search_decode(text, max_length=default_max_length)
                    method = "æŸæœç´¢"
                else:
                    text = user_input
                    result = self.greedy_decode(text, max_length=default_max_length)
                    method = "è´ªå¿ƒ"
                
                print(f"è‹±è¯­ ({method}) >>> {result}")
                print()
                
            except KeyboardInterrupt:
                print("\nå†è§!")
                break
            except Exception as e:
                print(f"ç¿»è¯‘é”™è¯¯: {e}")



def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Transformeræ¨ç†')
    parser.add_argument('--checkpoint', '--model', required=True, help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data-dir', default='./wmt14_data', help='æ•°æ®ç›®å½•')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’æ¨¡å¼')
    parser.add_argument('--text', help='è¦ç¿»è¯‘çš„æ–‡æœ¬')
    parser.add_argument('--input-file', help='è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--output-file', help='è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--method', default='greedy', choices=['greedy', 'beam'], help='è§£ç æ–¹æ³•')
    parser.add_argument('--beam-size', type=int, default=4, help='æŸå¤§å°')
    parser.add_argument('--max-length', type=int, default=100, help='æœ€å¤§ç”Ÿæˆé•¿åº¦')
    parser.add_argument('--length-penalty', type=float, default=None, help='é•¿åº¦æƒ©ç½š (alpha)ï¼Œé»˜è®¤è¯»å–config')
    
    args = parser.parse_args()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    model, src_tokenizer, tgt_tokenizer, vocab_info, device = load_model_and_tokenizers(
        args.checkpoint, args.data_dir, device
    )
    
    # åˆ›å»ºæ¨ç†å™¨
    inference = TransformerInference(
        model, src_tokenizer, tgt_tokenizer, device,
        vocab_info['bos_token_id'], vocab_info['eos_token_id'], vocab_info['pad_token_id']
    )
    
    if args.interactive:
        # äº¤äº’æ¨¡å¼
        inference.interactive(args.max_length)
        
    elif args.text:
        # å•æ–‡æœ¬ç¿»è¯‘
        if args.method == 'greedy':
            result = inference.greedy_decode(args.text, args.max_length)
        else:
            result = inference.beam_search_decode(
                args.text, args.beam_size, args.max_length, args.length_penalty
            )
        
        print(f"è¾“å…¥: {args.text}")
        print(f"è¾“å‡º: {result}")
        
    elif args.input_file:
        # æ–‡ä»¶ç¿»è¯‘
        if not os.path.exists(args.input_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            return
        
        with open(args.input_file, 'r', encoding='utf-8') as f:
            # è¯»å–æ–‡ä»¶å¹¶è¿‡æ»¤ç©ºè¡Œ - åŸºç¡€å¾ªç¯ç‰ˆæœ¬
            texts = []
            for line in f:
                line_stripped = line.strip()
                if line_stripped:
                    texts.append(line_stripped)
        
        print(f"ç¿»è¯‘ {len(texts)} ä¸ªå¥å­...")
        
        # å‡†å¤‡å‚æ•° - åŸºç¡€ç‰ˆæœ¬
        if args.method == 'beam':
            results = inference.translate_batch(
                texts, args.method, args.max_length, args.beam_size, args.length_penalty
            )
        else:
            results = inference.translate_batch(
                texts, args.method, args.max_length
            )
        
        if args.output_file:
            with open(args.output_file, 'w', encoding='utf-8') as f:
                for result in results:
                    f.write(result + '\n')
            print(f"âœ… ç»“æœå·²ä¿å­˜: {args.output_file}")
        else:
            for src, tgt in zip(texts, results):
                print(f"{src} -> {tgt}")
    
    else:
        print("è¯·æŒ‡å®šç¿»è¯‘æ¨¡å¼: --interactive, --text, æˆ– --input-file")


if __name__ == "__main__":
    main()
