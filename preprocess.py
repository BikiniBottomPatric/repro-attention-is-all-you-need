#!/usr/bin/env python3
"""
é¢„å¤„ç†è„šæœ¬ - ç‹¬ç«‹è®­ç»ƒåˆ†è¯å™¨ï¼ˆä¸è®­ç»ƒå®Œå…¨è§£è€¦ï¼‰

åŸè®ºæ–‡ä½¿ç”¨ BPE (Byte Pair Encoding) åˆ†è¯ï¼Œè¯è¡¨å¤§å°çº¦ 37000ã€‚
æœ¬è„šæœ¬æ”¯æŒ SentencePiece BPEï¼ˆæ¨èï¼ŒåŸè®ºæ–‡ä½¿ç”¨ï¼‰å’Œ HuggingFace BPEã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python preprocess.py                    # ä½¿ç”¨config.pyä¸­çš„é…ç½®
    python preprocess.py --force            # å¼ºåˆ¶é‡æ–°è®­ç»ƒ
    python preprocess.py --vocab-size 32000 # è‡ªå®šä¹‰è¯è¡¨å¤§å°
"""

import os
import argparse


def train_sentencepiece(data_dir: str, vocab_size: int, model_type: str = 'bpe'):
    """è®­ç»ƒ SentencePiece åˆ†è¯å™¨ï¼ˆåŸè®ºæ–‡æ–¹å¼ï¼‰"""
    try:
        import sentencepiece as spm
    except ImportError:
        raise ImportError("éœ€è¦å®‰è£… sentencepiece: pip install sentencepiece")
    
    train_de = os.path.join(data_dir, 'train.de')
    train_en = os.path.join(data_dir, 'train.en')
    
    if not os.path.exists(train_de) or not os.path.exists(train_en):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_de} æˆ– {train_en}")
    
    print(f"ğŸ”¤ è®­ç»ƒ SentencePiece åˆ†è¯å™¨")
    print(f"   ç®—æ³•: {model_type.upper()}")
    print(f"   è¯è¡¨å¤§å°: {vocab_size}")
    
    # åˆ›å»ºåˆå¹¶è¯­æ–™æ–‡ä»¶
    corpus_path = os.path.join(data_dir, 'spm_corpus.txt')
    print(f"   åˆ›å»ºåˆå¹¶è¯­æ–™...")
    
    line_count = 0
    with open(corpus_path, 'w', encoding='utf-8') as f_out:
        with open(train_de, 'r', encoding='utf-8') as f_de:
            for line in f_de:
                text = line.strip().replace('\n', ' ')
                if text:
                    f_out.write(text + '\n')
                    line_count += 1
        with open(train_en, 'r', encoding='utf-8') as f_en:
            for line in f_en:
                text = line.strip().replace('\n', ' ')
                if text:
                    f_out.write(text + '\n')
                    line_count += 1
    
    print(f"   è¯­æ–™è¡Œæ•°: {line_count:,}")
    print(f"   è®­ç»ƒä¸­... (çº¦5-15åˆ†é’Ÿ)")
    
    # è®­ç»ƒ SentencePiece
    spm_prefix = os.path.join(data_dir, 'spm_shared')
    spm.SentencePieceTrainer.Train(
        input=corpus_path,
        model_prefix=spm_prefix,
        vocab_size=vocab_size,
        model_type=model_type,  # bpe | unigram
        character_coverage=1.0,
        # ç‰¹æ®Štoken IDï¼ˆä¸æ¨¡å‹ä»£ç ä¸€è‡´ï¼‰
        pad_id=0,   # <pad>
        unk_id=1,   # <unk>
        bos_id=2,   # <s>
        eos_id=3,   # </s>
    )
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(corpus_path)
    
    print(f"âœ… åˆ†è¯å™¨å·²ä¿å­˜:")
    print(f"   {spm_prefix}.model")
    print(f"   {spm_prefix}.vocab")
    
    # éªŒè¯
    sp = spm.SentencePieceProcessor(model_file=spm_prefix + '.model')
    print(f"   å®é™…è¯è¡¨å¤§å°: {sp.get_piece_size()}")
    print(f"   ç‰¹æ®Štoken: <pad>={sp.pad_id()}, <unk>={sp.unk_id()}, <s>={sp.bos_id()}, </s>={sp.eos_id()}")


def train_hf_bpe(data_dir: str, vocab_size: int):
    """è®­ç»ƒ HuggingFace BPE åˆ†è¯å™¨ï¼ˆå¤‡é€‰æ–¹å¼ï¼‰"""
    try:
        from tokenizers import Tokenizer
        from tokenizers import decoders
        from tokenizers.models import BPE
        from tokenizers.trainers import BpeTrainer
        from tokenizers.pre_tokenizers import ByteLevel as ByteLevelPreTokenizer
    except ImportError:
        raise ImportError("éœ€è¦å®‰è£… tokenizers: pip install tokenizers")
    
    train_de = os.path.join(data_dir, 'train.de')
    train_en = os.path.join(data_dir, 'train.en')
    
    if not os.path.exists(train_de) or not os.path.exists(train_en):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_de} æˆ– {train_en}")
    
    print(f"ğŸ”¤ è®­ç»ƒ HuggingFace BPE åˆ†è¯å™¨")
    print(f"   è¯è¡¨å¤§å°: {vocab_size}")
    
    special_tokens = ['<pad>', '<unk>', '<s>', '</s>']
    
    # åˆ›å»ºåˆ†è¯å™¨
    tok = Tokenizer(BPE(unk_token='<unk>'))
    tok.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False)
    
    trainer = BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=special_tokens,
        min_frequency=2,
    )
    
    # è¿­ä»£å™¨
    def text_iterator():
        with open(train_de, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    yield line.strip()
        with open(train_en, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    yield line.strip()
    
    print("   è®­ç»ƒä¸­... (çº¦5-15åˆ†é’Ÿ)")
    tok.train_from_iterator(text_iterator(), trainer)
    tok.decoder = decoders.ByteLevel()
    
    # ä¿å­˜
    save_path = os.path.join(data_dir, 'tokenizer_shared.json')
    tok.save(save_path)
    
    print(f"âœ… åˆ†è¯å™¨å·²ä¿å­˜: {save_path}")
    print(f"   å®é™…è¯è¡¨å¤§å°: {tok.get_vocab_size()}")


def main():
    # ä» config è¯»å–é»˜è®¤å€¼
    from config import get_config
    config = get_config()
    
    parser = argparse.ArgumentParser(description='é¢„å¤„ç† - è®­ç»ƒåˆ†è¯å™¨')
    parser.add_argument('--data-dir', default=config['data_dir'], 
                       help=f'æ•°æ®ç›®å½• (é»˜è®¤: {config["data_dir"]})')
    parser.add_argument('--backend', default=config.get('tokenizer_backend', 'sentencepiece'),
                       choices=['sentencepiece', 'bpe'],
                       help='åˆ†è¯å™¨åç«¯')
    parser.add_argument('--vocab-size', type=int, 
                       default=config.get('sp_vocab_size', config.get('vocab_size', 37000)),
                       help='è¯è¡¨å¤§å°')
    parser.add_argument('--model-type', default=config.get('sp_model_type', 'bpe'),
                       choices=['bpe', 'unigram'],
                       help='SentencePieceç®—æ³•ç±»å‹ (é»˜è®¤: bpeï¼ŒåŸè®ºæ–‡ä½¿ç”¨)')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°è®­ç»ƒï¼ˆåˆ é™¤å·²æœ‰åˆ†è¯å™¨ï¼‰')
    
    args = parser.parse_args()
    
    print("=" * 50)
    print("ğŸ“¦ Transformer é¢„å¤„ç† - åˆ†è¯å™¨è®­ç»ƒ")
    print("=" * 50)
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"åˆ†è¯å™¨åç«¯: {args.backend}")
    print(f"è¯è¡¨å¤§å°: {args.vocab_size}")
    if args.backend == 'sentencepiece':
        print(f"ç®—æ³•ç±»å‹: {args.model_type}")
    print()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    train_de = os.path.join(args.data_dir, 'train.de')
    train_en = os.path.join(args.data_dir, 'train.en')
    
    if not os.path.exists(train_de) or not os.path.exists(train_en):
        print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨!")
        print(f"   éœ€è¦: {train_de}")
        print(f"   éœ€è¦: {train_en}")
        return
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    spm_model = os.path.join(args.data_dir, 'spm_shared.model')
    bpe_json = os.path.join(args.data_dir, 'tokenizer_shared.json')
    
    if args.force:
        # å¼ºåˆ¶åˆ é™¤å·²æœ‰åˆ†è¯å™¨
        for f in [spm_model, spm_model.replace('.model', '.vocab'), bpe_json]:
            if os.path.exists(f):
                os.remove(f)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {f}")
    else:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if args.backend == 'sentencepiece' and os.path.exists(spm_model):
            print(f"âœ… åˆ†è¯å™¨å·²å­˜åœ¨: {spm_model}")
            print("   ä½¿ç”¨ --force å¼ºåˆ¶é‡æ–°è®­ç»ƒ")
            return
        if args.backend == 'bpe' and os.path.exists(bpe_json):
            print(f"âœ… åˆ†è¯å™¨å·²å­˜åœ¨: {bpe_json}")
            print("   ä½¿ç”¨ --force å¼ºåˆ¶é‡æ–°è®­ç»ƒ")
            return
    
    # è®­ç»ƒåˆ†è¯å™¨
    if args.backend == 'sentencepiece':
        train_sentencepiece(args.data_dir, args.vocab_size, args.model_type)
    else:
        train_hf_bpe(args.data_dir, args.vocab_size)
    
    print()
    print("ğŸ‰ é¢„å¤„ç†å®Œæˆï¼")
    print("   ä¸‹ä¸€æ­¥: python train.py")


if __name__ == '__main__':
    main()
